{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b481ef-b6ef-4dcb-a7f2-53ad355b7c7a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Gentle Introduction to Creating an English-to-Korean translator with Transformers\"\n",
    "author: \"Jo, Joonu\"\n",
    "date: \"2023-02-27\"\n",
    "image: translate.png\n",
    "categories: [ai, code, transformers]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b9fe1-fa93-4af7-9ccd-f872322c759e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 친절한 영어-한국어 번역기 만들기 A Gentle Introduction to Creating an English-to-Korean translator with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee13d5f-3387-4583-9ccc-c0640ed8d1ac",
   "metadata": {},
   "source": [
    "트랜스포머가 딥러닝 세상을 지배한 지금 과거 CNN, RNN을 모르고 딥러닝을 안다고 할 수 없듯이 이제는 트랜스포머를 이해하지 못하고 딥러닝을 공부한다고 말할 수 없는 시대가 되었다.\n",
    "\n",
    "트랜스포머가 초기 타겟한 작업이 번역이기 때문에 트랜스포머를 설명하는 글에서 단골로 등장하는 예제가 번역기 예제다. 주로 영어-독일어, 영어-스페인어 예제가 많다. 하지만 쉽게 접할 수 있는 알파벳권 언어 사이의 번역기 예제에 비해 영어-한국어 데이터를 사용해서 번역기를 학습시키는 예제는 이상하리만큼 찾아보기 힘들었다. 왜 그런지 이유는 잘 모르겠지만 어쨌든 거의 없다. 그래서 영어-한국어 문장쌍이 들어있는 원시데이터를 사용해 T5 모델로 영어-한국어 번역기를 데모 수준 정도로 학습하는 예제를 만들어 블로그에 포스팅하면 많은 사람들에게 도움이 되지 않을까 해서 이 글을 적게 되었다.\n",
    "\n",
    "이 글에서는 트랜스포머에 대한 기초적인 내용은 다루지 않고 오직 데이터를 어떻게 준비하고, 어떻게 데이터를 모델에 입력하여 학습을 시키고, 마지막으로 어떻게 영어로 부터 한국어 번역 문장을 출력시키는가 하는 것에만 초점을 맞추었다. 그리고 **코드를 복잡하게 만드는 그 어떤 테크닉도 사용하지 않는다.** 오로지 가장 간단하게 한국어 번역기를 구축하는데만 집중할 것이다. 사실 이 글의 대부분 내용은 허깅페이스 [도움말](https://huggingface.co/course/chapter7/4?fw=pt)에 있는 것을 정리한것에 지나지 않는다. 하지만 입문자나 이제 막 트랜스포머를 이용해서 한국어 번역기를 만들고자 하는 사람들은 허깅페이스 도움말을 보고 이 내용을 모두 정리하기 쉽지 않은 것이 사실이어서 이글이 꽤 도움이 되리라 생각한다.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/metamath1/ml-simple-works/blob/master/t5/gentle_t5_trans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decab538-1523-4245-acfd-677263628b46",
   "metadata": {},
   "source": [
    "## 필요 패키지 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22528325-7cb0-40b2-9361-99c5012b827e",
   "metadata": {},
   "source": [
    "시작하기전에 필요한 라이브러리를 설치한다. 본인 컴퓨터에 이미 관련 라이브러리가 설치되어 있다면 설치하지 않아도 된다.\n",
    "\n",
    "먼저 허깅페이스의 트랜스포머스 라이브러리를 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3d2579-3dae-408c-ae0e-091a2a84936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0432ced-9cb3-4988-b039-40857a8ba476",
   "metadata": {},
   "source": [
    "그 다음은 데이터 셋을 다운받는기 위해 다음 명령을 실행해서 허깅페이스 `Datasets` 라이브러리를 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e90ff00-2d99-48c6-b52d-8ddc8c781131",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ed04d0-e337-414e-84e9-c32bb8636c72",
   "metadata": {},
   "source": [
    "그리고 T5 모델의 토크나이저가 `sentencepiece`를 사용하므로 다음을 실행해서 설치한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c6e9ab-8836-4384-ae16-8d7938fac647",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57efd602-7c2a-41d0-b2fe-ba8ba0a6b04b",
   "metadata": {},
   "source": [
    "또 모델 평가를 위해 허깅페이스 `evaluate` 라이브러리와 BLEU 점수를 계산하기위해 `sacrebleu`를 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9594360f-5dd1-43f6-a4ea-d66663b8054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11aaca5-aaad-4d2c-a519-c337cd29e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be8e63-80b5-45f9-9f63-0a481777d09c",
   "metadata": {},
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2540e8-3fdb-4ca5-87f5-ea8963f9ffd8",
   "metadata": {},
   "source": [
    "모두 설치가 완료되었다면 데이터 셋을 다운받아야 한다. 먼저 허깅페이스 사이트에 접속해서 상단 메뉴에 Datasets를 클릭하고 아래처럼 검색 조건을 맞추면\n",
    "\n",
    "- 좌측 작은 메뉴에서 Languages를 선택한다.\n",
    "- Languages 하단에 보이는 여러 언어중에 Korean을 선택한다.\n",
    "- 다시 우측 검색 필터 창에 en을 적는다.\n",
    "\n",
    "데이터 셋 네 개가 보이는데 이 중에서 `bongsoo/news_talk_en_ko`를 사용하도록 하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256e4cd-8025-4600-a749-31ef2b32720a",
   "metadata": {},
   "source": [
    "`bongsoo/news_talk_en_ko`를 클릭해서 나오는 화면에서 `Files and Versions`를 클릭하면 tsv 파일이 보이는데 이 파일에는 영어 문장과 한국어 문장이 한줄에 탭 문자로 구분되어 적혀있다. 로컬 디스크이 이 파일을 다운받고 파일을 읽어보면 다음처럼 확인된다. \n",
    "\n",
    "> [노트] 로컬 또는 코랩 런타임에 파일을 다운 받지 않았다면 굳이 다운받을 필요없고 이 셀은 스킵하자. 그냥 데이터 파일 한줄에 영어 문장과 짝이 되는 한국어 문장이 탭문자로 구분되어 있다는 것만 알면 된다. 실제 데이터를 가져올 때는 허깅페이스를 통해 다운 받게 된다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf899d4-94e2-44da-be99-dd3bd175785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skinner's reward is mostly eye-watering.\t스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.\n",
      "Even some problems can be predicted.\t심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.\n",
      "Only God will exactly know why.\t오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.\n",
      "Businesses should not overlook China's dispute.\t중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.\n",
      "Slow-beating songs often float over time.\t박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.\n"
     ]
    }
   ],
   "source": [
    "!head -5 news_talk_en_ko_train_130000.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df87bf1-49d0-4f34-828e-ffadcd58ca06",
   "metadata": {},
   "source": [
    "데이터 파일은 아주 단순한 형태인 것을 알 수 있다. 직접 tsv파일을 다운받아서 사용해도 되나 허깅페이스 허브로 부터 바로 다운받아 사용하는 편이 더 편하다. 다음 명령으로 다운받을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93bf4a49-43d5-4bd2-8369-8a0b395a2e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋을 다운받을 함수를 임포트 한다.\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5956b760-f263-4569-b4e6-5f88ec235cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration bongsoo--news_talk_en_ko-e7f00bc8f76f18d5\n",
      "Found cached dataset csv (/home/metamath/.cache/huggingface/datasets/bongsoo___csv/bongsoo--news_talk_en_ko-e7f00bc8f76f18d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8581b36fa89d4511a079d1086c126c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 좀 전에 알아본 체크포인트를 사용해서 데이터를 받아온다.\n",
    "en_ko = load_dataset(\"bongsoo/news_talk_en_ko\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00671055-4398-4306-9475-f49f773d8425",
   "metadata": {},
   "source": [
    "이제 데이터 객체를 확인해보면 `DatasetDict`라는 것을 알 수 있고 안에 `train` 키만 있는 것이 확인된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69638a74-2941-4a05-aa41-37ccaf8166e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: [\"Skinner's reward is mostly eye-watering.\", '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.'],\n",
       "        num_rows: 1299999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943f0d6-7eba-4d3f-9ef1-bccb85c2830c",
   "metadata": {},
   "source": [
    "`train`키에 `Dataset` 객체가 하나 있는데 `features`가 첫번째 데이터로 되어있고 행수는 1299999개인 것을 보아 데이터 파일에 컬럼명이 적혀있는 헤더라인이 없어서 첫줄을 헤더로 읽은것 같다. 첫줄을 데이터로 다시 집어 넣고 컬럼명은 `en`, `ko`로 설정하기 위해 데이터 셋을 `pandas`로 읽어드린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd46899-4aea-435d-b480-8dea8adc2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d1cd4b2-40de-40e5-82ae-85b5af5fe19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 데이터셋을 판다스 포맷으로 세팅\n",
    "en_ko.set_format(type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34946bd2-2ce7-4773-94de-656a405fc122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skinner's reward is mostly eye-watering.</th>\n",
       "      <th>스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Even some problems can be predicted.</td>\n",
       "      <td>심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Only God will exactly know why.</td>\n",
       "      <td>오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Businesses should not overlook China's dispute.</td>\n",
       "      <td>중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Slow-beating songs often float over time.</td>\n",
       "      <td>박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I can't even consider uninsured treatments.</td>\n",
       "      <td>보험 처리가 안 되는 비급여 시술은 엄두도 못 낸다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Skinner's reward is mostly eye-watering.  \\\n",
       "0             Even some problems can be predicted.   \n",
       "1                  Only God will exactly know why.   \n",
       "2  Businesses should not overlook China's dispute.   \n",
       "3        Slow-beating songs often float over time.   \n",
       "4      I can't even consider uninsured treatments.   \n",
       "\n",
       "     스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.  \n",
       "0  심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.  \n",
       "1      오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.  \n",
       "2    중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.  \n",
       "3     박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.  \n",
       "4       보험 처리가 안 되는 비급여 시술은 엄두도 못 낸다.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'train'키의 모든 행을 DataFrame df에 할당\n",
    "df = en_ko[\"train\"][:]\n",
    "\n",
    "# 잘 담겼는지 확인한다.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a0992-16a4-4bc3-8c9d-001359559bbd",
   "metadata": {},
   "source": [
    "예상처럼 첫 줄이 헤더가 되었으니 이를 수정한 `DataFrame`을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e15b742-e808-4b15-99a0-28b4bb5d09dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Skinner's reward is mostly eye-watering.\",\n",
       " '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_0 = list(df.columns)\n",
    "example_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42cbd3-8748-46f6-9dc2-80462bb881c2",
   "metadata": {},
   "source": [
    "적당히 조작해서 컬럼명이 `en`, `ko`가 되게 하고 `example_0`가 첫 행이 되도록 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cfe588c-5cbf-4b33-8c0e-14a72277b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_0_df = pd.DataFrame({col: [value] for col, value in zip(('en', 'ko'), example_0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b61a1c-b538-4051-a5d5-757ea1cce1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ('en', 'ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d16fbbc-1baf-47a1-b15b-abddf9b372e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skinner's reward is mostly eye-watering.</td>\n",
       "      <td>스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Even some problems can be predicted.</td>\n",
       "      <td>심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Only God will exactly know why.</td>\n",
       "      <td>오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Businesses should not overlook China's dispute.</td>\n",
       "      <td>중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slow-beating songs often float over time.</td>\n",
       "      <td>박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                en  \\\n",
       "0         Skinner's reward is mostly eye-watering.   \n",
       "1             Even some problems can be predicted.   \n",
       "2                  Only God will exactly know why.   \n",
       "3  Businesses should not overlook China's dispute.   \n",
       "4        Slow-beating songs often float over time.   \n",
       "\n",
       "                                   ko  \n",
       "0    스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.  \n",
       "1  심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.  \n",
       "2      오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.  \n",
       "3    중국의 논쟁을 보며 간과해선 안 될 게 기업들의 고충이다.  \n",
       "4     박자가 느린 노래는 오랜 시간이 지나 뜨는 경우가 있다.  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ko_df = pd.concat([example_0_df, df],).reset_index(drop=True)\n",
    "en_ko_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc5662-faf2-426a-aa6a-afe80d7b19a6",
   "metadata": {},
   "source": [
    "이렇게 데이터 셋을 `DataFrame`으로 만들었다. 이제 이 `en_ko_df`로 부터 다시 허깅페이스 데이터 셋을 생성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "681f6ed9-d488-47cb-a24a-23626146a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0631eb3d-ee3d-4264-89dd-2395a1bee4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(en_ko_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb85dddc-0e58-4418-8181-83daba3c6049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ko'],\n",
       "    num_rows: 1300000\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6f02a-8537-4d36-b65e-0d396cf151a9",
   "metadata": {},
   "source": [
    "다시 데이터 셋을 확인해보면 `features`가 제대로 표시되고 샘플 수도 1300000개 인것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05c2bb-f303-46d3-8302-89f37549f441",
   "metadata": {},
   "source": [
    "이렇게 만들어진 `DataFrame`으로 부터 데이터 셋이 잘 초기화되는 것을 확인했으니 `en_ko_df`를 세조각으로 쪼개서 tsv파일로 저장하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "604f23cd-d95c-4337-be34-3a09635e61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터 셋의 샘플수를 정한다.\n",
    "num_train = 1200000\n",
    "num_valid = 90000\n",
    "num_test = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea4112c-c615-4f51-9297-0f73c4ae6290",
   "metadata": {},
   "source": [
    "설정된 크기만큼 `DataFrame`을 자른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27a055e4-0df6-449f-899d-6146cf2abe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_df_train = en_ko_df.iloc[:num_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0651dd95-6095-413a-b19b-a839c3b59a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_df_valid = en_ko_df.iloc[num_train:num_train+num_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e662e9-9d1e-4103-8780-fc6dac2bd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_df_test = en_ko_df.iloc[-num_test:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011f014-cac7-47d5-873e-70ab59a39e04",
   "metadata": {},
   "source": [
    "다시 `tsv`파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cb755cb-d83e-4c2a-a92b-54d0cf58fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_df_train.to_csv(\"train.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc2a51b7-f69b-4b80-8e1d-d3d9f60c227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_df_valid.to_csv(\"valid.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f0108ff-5aa4-4325-8b66-6a07804bd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_df_test.to_csv(\"test.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6f650-7084-49c0-a6a3-3d32ef14f46a",
   "metadata": {},
   "source": [
    "이렇게 `tsv`파일 세개로 데이터를 정리했다. 이제 필요할때 이 파일을 읽어 허깅페이스 데이터셋을 만들 수 있다.\n",
    "\n",
    "아래처럼 스플릿을 정의한 사전을 `load_dataset`에 넘기면 된다. 이때 `delimiter`를 탭 문자로 지정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d99b4a2-78d6-4984-8207-b87ab4c512be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"train.tsv\", \"valid\": \"valid.tsv\", \"test\": \"test.tsv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6b8272a-0351-4305-b18a-4529ec2f9035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-02a3611b1810efcd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fb1537de8b4b658f2795aa5ac95733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad38390f5d545e1916119fc6e3266e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/metamath/.cache/huggingface/datasets/csv/default-02a3611b1810efcd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f710594596b34c59820df0eb222990e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset =  load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0922f1-ea9d-404c-95b0-c72edebc0adb",
   "metadata": {},
   "source": [
    "제대로 로딩되었는지 `dataset`을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7e84925-56f0-4374-a756-f35a5057bd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ko'],\n",
       "        num_rows: 1200000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['en', 'ko'],\n",
       "        num_rows: 90000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'ko'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a46daa-863e-4438-a848-742ee3638ed7",
   "metadata": {},
   "source": [
    "`DatasetDict`에 `train`, `valid`, `test` 키로 120만 문장, 9만 문장, 1만 문장이 저장된 것을 확인할 수 있다.\n",
    "\n",
    "이 데이터 셋에서 개별 샘플에 대한 접근은 `[split][feature][row num]` 형태로 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cf854ed-5b0d-444b-b35e-d1f1e5b58e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n"
     ]
    }
   ],
   "source": [
    "# train 스플릿에서 영어 3개와 한국어 3개 샘플을 가져온다.\n",
    "print(dataset['train']['en'][:3], dataset['train']['ko'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd4cf6-fe95-4ab5-ac6e-7539cc1fe594",
   "metadata": {},
   "source": [
    "그런데 `feature`와 `row num`은 순서를 바꿔서 사용할 수 도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70014441-e8a8-496c-bf3f-a40b7c91a498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Skinner's reward is mostly eye-watering.\", 'Even some problems can be predicted.', 'Only God will exactly know why.'] ['스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.', '심지어 어떤 문제가 발생할 건지도 어느 정도 예측이 가능하다.', '오직 하나님만이 그 이유를 제대로 알 수 있을 겁니다.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:3]['en'], dataset['train'][:3]['ko'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89453b75-80b2-40f2-a0ba-ca3977138449",
   "metadata": {},
   "source": [
    "데이터를 어떻게 조회하는지는 데이터 구성 방식에 따라 조금씩 다르므로 데이터 셋을 보고 몇번 해보면 금방 접근법을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d3379-b806-4e46-90c3-fcc3691c03ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hugging face\n",
    "\n",
    "데이터 셋 준비를 마쳤으니 학습할 차례이다. 허깅페이스에서 제공하는 필요 클래스를 임포트 한다.\n",
    "\n",
    "먼저 선학습 모델을 사용하기 위한 클래스를 임포트 한다. `AutoTokenizer`는 선학습된 모델이 사용한 토크나이저를 읽기 위해 필요하며 `AutoModelForSeq2SeqLM`은 시퀀스 투 스퀀스 방식으로 작동하는 선학습된 모델을 불러 올 때 마지막에 분류기 헤드를 붙여서 모델을 로딩하기 위해 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91addd01-dae9-4367-a6ea-560d7c6761b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 16:05:02.191320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-01 16:05:02.266647: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-01 16:05:02.281905: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-01 16:05:02.592853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-03-01 16:05:02.592895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-03-01 16:05:02.592898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc4b0ba-f3a8-4f09-9131-007ab06cc855",
   "metadata": {},
   "source": [
    "다음은 데이터 콜레이터를 임포트한다. 시쿼스 투 시퀀스 학습 과정은 인코더 입력 시퀀스, 디코더 입력 시퀀스, 디코더 출력 시퀀스를 필요로 하는데 미니배치로 부터 이를 적절히 정리해서 모델에 입력하는 작업이 필요하다. 예를 들면 미니 배치 내에 있는 인코더 입력 시퀀스의 길이를 맞춘다든지 디코더 입력시퀀스를 오른쪽으로 한칸 쉬프트시켜 디코더 출력 시퀀스를 만드는 작업등이 콜레이터에서 일어나는 작업인데 이런 작업을 `DataCollatorForSeq2Seq`가 자동으로 처리하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "330f2560-3457-4c64-907f-63dc78246208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661209be-091b-410d-9e13-4ebfff38e633",
   "metadata": {},
   "source": [
    "그리고 학습에 필요한 클래스를 임포트 한다. 학습에 필요한 설정을 `Seq2SeqTrainingArguments`에 정의하고 실제 학습은 `Seq2SeqTrainer`로 하게 된다.\n",
    "`Seq2SeqTrainer`는 `generate()`함수를 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "301e0129-a447-4949-b572-5fe605702707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11029e-0c2a-4437-99d2-41aa1c6acce3",
   "metadata": {},
   "source": [
    "허깅페이스 라이브러리로는 마지막으로 데이터 셋을 로딩하는 함수와 번역 결과를 측정할 함수를 로딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9741b79e-1bc9-4f3b-8163-820f237fdc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ef6f3-a0ea-4d8e-bcc7-aaf4f89d8de8",
   "metadata": {},
   "source": [
    "그외 필요한 각종 라이브러리를 임포트 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1354b5d1-17f3-4b2a-a62b-8098855b712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704916ef-74a8-4329-a4a2-4559f5c781a6",
   "metadata": {},
   "source": [
    "허깅페이스에서 파이토치 기반 구현을 사용하므로 gpu가 있다면 `device`를 세팅한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac2f6992-a762-474f-a879-acfb6f106bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad4fcb-bf9b-4b48-bc94-9f8d58c24970",
   "metadata": {},
   "source": [
    "미리 학습된 모델의 체크포인트를 세팅한다. 여기서 사용할 모델은 한국어와 영어에 미리 학습된 [KE-T5](https://github.com/AIRC-KETI/ke-t5)모델을 사용한다. T5모델은 트랜스포머의 인코더, 디코더 구조를 모두 사용하는 모델로 번역기를 만들 때 사용할 수 있는 모델이다. 아래처럼 모델 체크 포인트와 T5 모델에 입력될 최대 토큰 길이를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a354a6f-8561-4b4b-82a5-37e23684de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"KETI-AIR/ke-t5-base\"\n",
    "max_token_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e219e4e-e336-4d5f-ab51-cbe84c824937",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "먼저 모델 체크 포인트를 사용하여 KE-T5 모델이 학습할때 함께 사용한 토크나이저를 불러온다. 허깅페이스 트랜스포머스 라이브러리를 사용할 때 가장 핵심이 되며 익숙해지기 쉽지 않은 부분이 이 토크나이저라고 개인적으로 생각한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0995a847-c994-40cd-9842-14124cbc91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0cc164-b77e-4c69-86ee-504f04462b49",
   "metadata": {},
   "source": [
    "토크나이저를 로딩할때 `sentencepiece`가 없다고 에러가 나면 위 제시한 라이브러리가 설치 안된 것이므로 설치하고 다시 시도한다.\n",
    "\n",
    "토크나이저를 불러왔으니 현재 사용하는 데이터셋에서 샘플을 가져와 토크나이징해보는 것이 좋을 것이다. 학습 세트에서 10번 샘플을 가지고 실험해보자. 먼저 10번 샘플을 뿌려보고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c2c8534-348e-4d94-9f0c-e252a52e666e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Any academic achievement requires constant repetition.',\n",
       " '어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10]['en'], dataset['train'][10]['ko']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4e727-e02e-407b-97ef-6f09ae9798b1",
   "metadata": {},
   "source": [
    "토크나이저에 각 문장을 입력하고 토큰화된 상태로 돌려 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ec77c5c-001e-46c5-b9e5-74a243a6b676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sample_en = tokenizer(dataset['train'][10]['en'], \n",
    "                                max_length=max_token_length, \n",
    "                                padding=True, truncation=True)\n",
    "tokenized_sample_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "686978da-d8bc-427c-9ae7-2d41a940a7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sample_ko = tokenizer(dataset['train'][10]['ko'], \n",
    "                                max_length=max_token_length, \n",
    "                                padding=True, truncation=True)\n",
    "tokenized_sample_ko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c77bca-c4ea-486a-aaba-4bc7a18b1264",
   "metadata": {},
   "source": [
    "문장에 토큰으로 쪼개지고 각 토큰이 숫자로 변환된 것을 볼 수 있다. 이렇게 숫자화된 토큰을 `input_ids`로 반환하고 추가로 트랜스포머 인코더, 디코더에 쓰일 패딩 마스크도 함께 `attention_mask`로 돌려준다. 마스크가 모두 1인 이유는 샘플이 하나밖에 없어서 이다. 샘플 몇개를 더 실험해보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8232db64-a06f-478f-8452-02c790bc85ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[388, 6809, 2952, 17, 8, 32204, 43, 8023, 6687, 28, 9495, 91, 3, 1], [4014, 322, 3170, 147, 67, 23274, 3, 1, 0, 0, 0, 0, 0, 0], [11783, 4412, 96, 6556, 709, 1632, 3, 1, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train'][:3]['en'], \n",
    "          max_length=max_token_length, \n",
    "          padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaccc9d-b892-4a5e-82bf-32b28426a685",
   "metadata": {},
   "source": [
    "미니배치에 있는 샘플의 최대길이메 맞춰서 패딩되는 모습을 확인할 수 있다. 실제로 어떻게 토큰화 되었는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6808b411-05b9-45d1-bccf-bb2ca78d345e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <td>13941</td>\n",
       "      <td>10114</td>\n",
       "      <td>25542</td>\n",
       "      <td>9361</td>\n",
       "      <td>20526</td>\n",
       "      <td>742</td>\n",
       "      <td>32268</td>\n",
       "      <td>12520</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁Any</td>\n",
       "      <td>▁academic</td>\n",
       "      <td>▁achievement</td>\n",
       "      <td>▁requires</td>\n",
       "      <td>▁constant</td>\n",
       "      <td>▁re</td>\n",
       "      <td>pet</td>\n",
       "      <td>ition</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1             2          3          4    5      6  \\\n",
       "ids     13941      10114         25542       9361      20526  742  32268   \n",
       "tokens   ▁Any  ▁academic  ▁achievement  ▁requires  ▁constant  ▁re    pet   \n",
       "\n",
       "            7  8     9  \n",
       "ids     12520  3     1  \n",
       "tokens  ition  .  </s>  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        tokenized_sample_en['input_ids'],\n",
    "        tokenizer.convert_ids_to_tokens(tokenized_sample_en['input_ids'])\n",
    "    ], index=('ids', 'tokens')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "693b6a83-1db1-4463-9b90-a0f2053080d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <td>404</td>\n",
       "      <td>12663</td>\n",
       "      <td>15</td>\n",
       "      <td>10775</td>\n",
       "      <td>2334</td>\n",
       "      <td>6</td>\n",
       "      <td>15757</td>\n",
       "      <td>21</td>\n",
       "      <td>29819</td>\n",
       "      <td>1736</td>\n",
       "      <td>26778</td>\n",
       "      <td>4342</td>\n",
       "      <td>15</td>\n",
       "      <td>1701</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁어떤</td>\n",
       "      <td>▁학문</td>\n",
       "      <td>이</td>\n",
       "      <td>든지</td>\n",
       "      <td>▁일정</td>\n",
       "      <td>의</td>\n",
       "      <td>▁성취</td>\n",
       "      <td>를</td>\n",
       "      <td>▁이루기</td>\n",
       "      <td>▁위해서는</td>\n",
       "      <td>▁끊임없는</td>\n",
       "      <td>▁반복</td>\n",
       "      <td>이</td>\n",
       "      <td>▁필요하다</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1   2      3     4  5      6   7      8      9      10    11  \\\n",
       "ids     404  12663  15  10775  2334  6  15757  21  29819   1736  26778  4342   \n",
       "tokens  ▁어떤    ▁학문   이     든지   ▁일정  의    ▁성취   를   ▁이루기  ▁위해서는  ▁끊임없는   ▁반복   \n",
       "\n",
       "        12     13 14    15  \n",
       "ids     15   1701  3     1  \n",
       "tokens   이  ▁필요하다  .  </s>  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        tokenized_sample_ko['input_ids'],\n",
    "        tokenizer.convert_ids_to_tokens(tokenized_sample_ko['input_ids'])\n",
    "    ], index=('ids', 'tokens')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2e2ff-83bb-4bb6-9a44-2759bd4a487d",
   "metadata": {},
   "source": [
    "KE-T5를 학습할때 학습된 규칙대로 토큰화가 진행된다. 영어에서 `repetition`은 `re`, `pet`, `ition`으로 쪼개진 것을 볼 수 있고, 한국어에서 `성취를`은 `성취`, `를`로 쪼개지고 `학문이든지`는 `학문`, `이`, `든지`로 쪼개진것을 볼 수 있다. 토큰 앞에 _표시는 이 토큰 앞에는 공백이 있어야 한다는 의미다. 그리고 마지막에 엔드 토큰인 `</s>`가 항상 붙게 되는 것도 확인할 수 있다.\n",
    "\n",
    "이제 앞서 tsv파일로 부터 로딩한 `dataset`내의 문장을 모두 토크나이저를 사용해서 숫자로 바꾸는 작업을 해야 한다. 즉 문자로된 문장을 숫자로 바꿔 특성화 해야 한다. `dataset.map()`함수에 각 샘플을 토큰화 하는 함수를 만들어 전달하면 `map()`이 모든 샘플에 대해 전달받은 함수를 적용하게 되는데 함수는 이렇게 작성하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbe9dd2f-ac05-42f0-b9ec-71589bc3d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples):\n",
    "    ###########################################################################\n",
    "    # with 쓰는 옛날 방식\n",
    "    # input_encodings = tokenizer(examples['en'], \n",
    "    #                             max_length=max_token_length, truncation=True)\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    # with tokenizer.as_target_tokenizer():\n",
    "    # target_encodings = tokenizer(text_target=examples['ko'], \n",
    "    #                             max_length=max_token_length, truncation=True)\n",
    "    #\n",
    "    #\n",
    "    # return {\n",
    "    #     \"input_ids\": input_encodings[\"input_ids\"],\n",
    "    #     \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "    #     \"labels\": target_encodings[\"input_ids\"]\n",
    "    # }\n",
    "    \n",
    "    # 그런데 이렇게 하면 인풋하고 한번에 처리 가능함.\n",
    "    model_inputs = tokenizer(examples['en'],\n",
    "                             text_target=examples['ko'], \n",
    "                             max_length=max_token_length, truncation=True)\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e84c49-8e0b-46d7-aada-bc84afc0e5cc",
   "metadata": {},
   "source": [
    "`convert_examples_to_features()`가 하고 싶은 일은 `dataset`에 있는 \"어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.\"라는 샘플 문장을 `[404,12663,15,10775,2334,6,15757,21,29819,1736,26778,4342,15,1701,3,1]`라는 정수로 바꾸는 것이다. `convert_examples_to_features()`가 `dataset`에 적용될 때 넘겨 받는 `examples`는 다음과 같이 넘어 온다.\n",
    "\n",
    "```python\n",
    "examples= {'en':['sent1', 'sent2', ... , 'sent1000'], # 이건 문장 1000개짜리 리스트\n",
    "           'ko':['sent1', 'sent2', ... , 'sent1000']}\n",
    "```\n",
    "\n",
    "기본으로 미니 배치 사이즈는 1000으로 세팅되어 있다.(함수 기본인자는 [여기](https://huggingface.co/docs/datasets/v2.9.0/en/package_reference/main_classes#datasets.Dataset.map)서 확인 가능) \n",
    "\n",
    "미니 배치로 넘어온 문장 샘플을 영어 문장과 한국어 문장을 각각 인풋과 타겟으로 토큰화하고 이로 부터 `input_ids`, `attention_mask`, `labels`로 묶어 리턴하는 방식이 예전에 쓰던 방식으로 함수 위쪽에 주석처리 되어 있다. 타겟 문장을 토큰화 할 때 타겟에서 필요로 하는 특수 토큰을 추가하는 경우 이를 처리하기위해 타겟 토큰 토큰화 때는 `with tokenizer.as_target_tokenizer():`라는 컨텍스트 매니저를 사용했는데 최근 업데이트에서는 그냥 `tokenizer`에 `text_target`인자에 타겟 문장을 넣어서 한번에 다 처리할 수 있다. 이렇게 `model_inputs`을 반환하면 `dataset`에 있던 각 레코드 마다 `en`, `ko` 특성에 추가로 `input_ids`, `attention_mask`, `labels` 특성이 더 추가 되게 된다. 사실 `en`, `ko` 특성은 더이상 필요없기 때문에 `convert_examples_to_features()`를 적용할 때 없애라는 인자를 세팅한다. \n",
    "\n",
    "바로 `dataset`에 함수를 적용해보자. 그냥 해도되나 좀 더 빠르게 하기 위해 `num_proc` 인자에 스레드 개수를 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c91dd404-676b-4898-ac3d-9f3a418b3d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CPU = multiprocessing.cpu_count() \n",
    "NUM_CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f02eb4-abf5-42e9-8fa7-77c49bb0d49f",
   "metadata": {},
   "source": [
    "그리고 `remove_columns` 인자에 기존 특성 이름인 `en`, `ko`를 전달해서 기존 특성은 제거하게 한다. 이 특성이 있으면 이후 콜레이터가 샘플들을 미니 배치로 묶을 때 패딩처리를 못하게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ed9cd-9d74-495e-8102-f71c41bd9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(convert_examples_to_features, \n",
    "                                 batched=True, \n",
    "                                 # 이걸 쓰지 않으면 원 데이터 'en', 'ko'가 남아서\n",
    "                                 # 아래서 콜레이터가 패딩을 못해서 에러남\n",
    "                                 remove_columns=dataset[\"train\"].column_names,\n",
    "                                 num_proc=NUM_CPU) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79a4c7-77de-4233-bfb1-894dcb08e995",
   "metadata": {},
   "source": [
    "> [노트] `dataset.map()`이 실행되면서 출력되는 출력은 생략됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ec637-0066-4b64-9c11-874c3cdfcacf",
   "metadata": {},
   "source": [
    "`convert_examples_to_features()`이 `dataset`의 모든 샘플에 다 적용되고 나면 `tokenized_datasets`는 다음처럼 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "357697dd-10fe-4319-9065-695ffaf365fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1200000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 90000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05054ee-57ad-4a0d-96d5-90555e34e342",
   "metadata": {},
   "source": [
    "기존에 있던 특성 `en`, `ko`는 사라졌고 `en`은 `input_ids`와 `attention_mask`로 `ko`는 `labels`로 바뀐것을 확인할 수 있다. 예를 들어 학습 세트에 10번 데이터를 보면 다음처럼 다 숫자라 바뀌게 된것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "761f5fa1-8cc8-4f5d-b730-8d34f60b9ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [404,\n",
       "  12663,\n",
       "  15,\n",
       "  10775,\n",
       "  2334,\n",
       "  6,\n",
       "  15757,\n",
       "  21,\n",
       "  29819,\n",
       "  1736,\n",
       "  26778,\n",
       "  4342,\n",
       "  15,\n",
       "  1701,\n",
       "  3,\n",
       "  1]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca3d7c-a8de-458b-8596-d3213f54713a",
   "metadata": {},
   "source": [
    "토크나이저를 써서 숫자로 부터 토큰화 해보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "262eafbc-6499-4862-aaaf-b539d22df130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원 데이터    : Any academic achievement requires constant repetition.\n",
      "처리 후 데이터: [13941, 10114, 25542, 9361, 20526, 742, 32268, 12520, 3, 1]\n",
      "토큰화       : ['▁Any', '▁academic', '▁achievement', '▁requires', '▁constant', '▁re', 'pet', 'ition', '.', '</s>']\n",
      "\n",
      "\n",
      "원 데이터    : 어떤 학문이든지 일정의 성취를 이루기 위해서는 끊임없는 반복이 필요하다.\n",
      "처리 후 데이터: ['▁어떤', '▁학문', '이', '든지', '▁일정', '의', '▁성취', '를', '▁이루기', '▁위해서는', '▁끊임없는', '▁반복', '이', '▁필요하다', '.', '</s>']\n",
      "토큰화       : [404, 12663, 15, 10775, 2334, 6, 15757, 21, 29819, 1736, 26778, 4342, 15, 1701, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "print( '원 데이터    :', dataset['train'][10]['en'] )\n",
    "print( '처리 후 데이터:', tokenized_datasets['train'][10]['input_ids'] )\n",
    "print( '토큰화       :', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['input_ids']) )\n",
    "\n",
    "print('\\n')\n",
    "print( '원 데이터    :', dataset['train'][10]['ko'] )\n",
    "print( '처리 후 데이터:', tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][10]['labels']) )\n",
    "print( '토큰화       :', tokenized_datasets['train'][10]['labels'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb05947-a1b0-498f-9ae2-35a11c47e723",
   "metadata": {},
   "source": [
    "데이터 특성화를 모두 마쳤으므로 이제 모델을 로딩하자. `AutoModelForSeq2SeqLM`를 사용해서 선학습 모델을 불러오면 선학습된 T5모델 마지막에 파인튜닝할 수 있는 분류 헤드를 붙인 모델을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba2636-1b8c-415c-9a19-b2ffeed589c4",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82e7ec24-49ea-457a-bfcb-73fed79a9766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488ebba-aa28-4c85-8ad7-ad0d9c5cd756",
   "metadata": {},
   "source": [
    "위처럼 모델을 로딩하고 모델 출력 시켜보면 T5 모델 레이어가 매우 길게 출력되는데 제일 마지막 부분에 다음과 같이 분류 헤드가 붙어 있는 것을 확인할 수 있다. 헤드를 보면 모델에서 출력하는 벡터는 768차원이고 이를 단어장 사이즈인 64128로 변환시키고 있는 것을 알 수 있다.\n",
    "\n",
    "```\n",
    "(lm_head): Linear(in_features=768, out_features=64128, bias=False)\n",
    "```\n",
    "\n",
    "이렇게 생성된 `model`은 인코더-디코더 구조를 가지는 트랜스포머이므로 이 모델을 포워딩 하려면 인코더 인풋과 디코더 인풋을 넣어줘야 한다. 모델을 만들고 가장 먼저해야되는 작업은 포워딩 테스트라고 개인적으로 생각한다. 임의의 입력을 넣고 출력이 의도대로 나오는지 확인하는 것이다. 이런 작업은 직접 만든 모델이 아닐 수록 중요한데 이렇게 해야지 모델이 제대로 작동하는지 또 어떤 구조로 되어 있는지 쉽게 이해할 수 있기 때문이다. 포워드 테스트를 하기위해 간단한 영어문장으로 예제를 준비한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c84561f1-678d-4ae4-ab36-91ca3766eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tokenizer(\n",
    "    [\"Studies have been shown that owning a dog is good for you\"], \n",
    "    return_tensors=\"pt\"\n",
    ")['input_ids'].to(device)\n",
    "\n",
    "decoder_targets = tokenizer(\n",
    "    [\"개를 키우는 것이 건강에 좋다는 연구 결과가 있습니다.\"], \n",
    "    return_tensors=\"pt\"\n",
    ")['input_ids'].to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003763c-759a-4dd0-8c67-96d46d2c49be",
   "metadata": {},
   "source": [
    "영어 문장은 인코더의 입력이 되고 한국어 문장은 디코더의 타겟이 된다. 아래처럼 모두 숫자로 변환되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7d0898a-451f-4a73-97d6-0fa51699df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24611,    84,   166,  8135,    38,   847,    91,    16,  8146,    43,\n",
      "           667,    40,   106,     1]], device='cuda:0')\n",
      "tensor([[15833, 12236,   179, 16120, 28117,  1007,  3883,   327,     3,     1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print( encoder_inputs )\n",
    "print( decoder_targets )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb132c9-959a-40ab-9795-84001af7ff6c",
   "metadata": {},
   "source": [
    "이제 디코더 입력을 만들기위해 `model._shift_right`를 사용해 디코더 출력을 오른쪽으로 쉬프트 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19c4ea3a-d846-4666-97ad-68f36cddf5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = model._shift_right(decoder_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba7551-19a7-43db-85d0-d79c61a48399",
   "metadata": {},
   "source": [
    "`decoder_inputs`와 `decoder_targets`이 어떻게 다른지 비교해보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "916784d6-724e-46a1-96d5-b2bc72c90c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>decoder target</th>\n",
       "      <td>▁개를</td>\n",
       "      <td>▁키우는</td>\n",
       "      <td>▁것이</td>\n",
       "      <td>▁건강에</td>\n",
       "      <td>▁좋다는</td>\n",
       "      <td>▁연구</td>\n",
       "      <td>▁결과가</td>\n",
       "      <td>▁있습니다</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decoder input</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>▁개를</td>\n",
       "      <td>▁키우는</td>\n",
       "      <td>▁것이</td>\n",
       "      <td>▁건강에</td>\n",
       "      <td>▁좋다는</td>\n",
       "      <td>▁연구</td>\n",
       "      <td>▁결과가</td>\n",
       "      <td>▁있습니다</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0     1     2     3     4     5     6      7      8     9\n",
       "decoder target    ▁개를  ▁키우는   ▁것이  ▁건강에  ▁좋다는   ▁연구  ▁결과가  ▁있습니다      .  </s>\n",
       "decoder input   <pad>   ▁개를  ▁키우는   ▁것이  ▁건강에  ▁좋다는   ▁연구   ▁결과가  ▁있습니다     ."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        tokenizer.convert_ids_to_tokens(decoder_targets[0]),\n",
    "        tokenizer.convert_ids_to_tokens(decoder_inputs[0])\n",
    "    ],\n",
    "    index=('decoder target', 'decoder input')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c83b08-077a-43bb-975c-f2df51071675",
   "metadata": {},
   "source": [
    "위처럼 오른쪽으로 쉬프트된 디코더 입력은 `<pad>` 토큰이 추가되었다. 이렇게 출력으로 쓰이는 문장을 오른쪽으로 쉬프트시켜 티처포싱Teacher forcing을 진행하게 된다. 다음처럼 `model`에 인코더 입력, 디코더 입력, 디코더 타겟을 입력하고 포워드 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f47e2f4b-3f1a-47da-8b81-6b90e992aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "outputs = model(input_ids=encoder_inputs, \n",
    "                decoder_input_ids=decoder_inputs, \n",
    "                labels=decoder_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8b6f9-80b0-4d07-9b3a-ef197883b986",
   "metadata": {},
   "source": [
    "`model`의 `outputs`에는 다음과 같은 키가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5084b428-d776-47fa-9c9c-f76bf3c99243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d660b-50eb-4f64-94cb-eda51f33c63f",
   "metadata": {},
   "source": [
    "손실함수 값을 다음처럼 확인할 수 있고 `grad_fn`이 있기 때문에 `output.loss`를 백워드 시킬 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d830740-8fda-47bc-858c-290ca1681f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(87.8185, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2227e-68eb-4a63-97e5-f034afbe536e",
   "metadata": {},
   "source": [
    "인코더의 마지막 상태는 (1, 14, 768)이다. 각 숫자는 순서대로 샘플 수, 스탭 수, 모델 사이즈를 나타낸다. 즉 인코더로 들어가는 14개 토큰이 각각 768차원 벡터로 인코딩되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d99482a-78c2-40f5-a563-2e8312d377b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['encoder_last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e681f2-851e-4ec4-aee6-223bed51dc26",
   "metadata": {},
   "source": [
    "`logit`은 디코더 입력 토큰 10개에 대한 그 다음 토큰 예측 10개를 담고있다. 샘플 한개에 대해서 10개 토큰에 대해서 64128개 단어에 대한 확률값이 들어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24ad8e3c-2910-4e04-bff5-693393ddb7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 64128])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefa5f3-654f-40c4-81cb-3e0b9145c64a",
   "metadata": {},
   "source": [
    "`logit`에 `argmax`를 씌워서 토큰화시켜보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "517afe80-a33c-4938-a07b-9c7e3024bf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['큐브', '큐브', '▁비일비재', '▁비일비재', '▁베네', '▁비일비재', '▁베네', '▁베네', '큐브', '큐브']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens( torch.argmax(outputs['logits'][0], axis=1).cpu().numpy() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1376410-1816-46a5-88be-4b09d009134d",
   "metadata": {},
   "source": [
    "마지막 헤더가 학습이 되지 않았기 때문에 적절한 아웃풋이 나오지 않지만 입력과 출력의 텐서 모양을 보면 포워드 패스가 제대로 작동한다는 것을 알 수 있다.\n",
    "\n",
    "지금까지 데이터 셋, 토크나이저, 모델에 대해서 알아봤다. 이제 학습을 위해 두 단계가 남았는데 하나는 데이터를 미니배치 형태로 모아 주는 콜레이터collator와 나머지 하나는 모델을 평가할 매트릭이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b7769-2eb5-4acd-987e-6563a499daa1",
   "metadata": {},
   "source": [
    "## Collator\n",
    "\n",
    "파이토치에서 모델을 학습시키기 위해서 `DataLoader`를 사용하게 되는데 이 데이터 로더의 역할은 for 루프를 돌면서 데이터 셋으로 부터 샘플을 미니 배치 수만큼 가져오는 것이다. 이때 샘플을 미니 배치 수만큼 무작위로 가져와 어떤 식으로든 각 샘플을 짝맞춤해서 반환해야하는데 크기가 통일된 간단한 이미지 데이터인 경우 특별히 할것이 없지만 서로 크기가 다른 샘플들을 다루는 경우는 반환전 크기 또는 길이를 맞춘다든지 패딩을 한다든지 하는 추가 작업이 필요하게 된다. 이런 작업이 일어나는 곳이 `collate_fn`으로 지정되는 함수이다. \n",
    "\n",
    "시퀀스 투 시퀀스 모델을 학습시킬때 이런 콜레이터 함수가 하는 전형적인 역할은 입력 또는 출력 문자열을 패딩하고 조금 전 모델에서 알아봤듯이 디코더 타겟을 오른쪽으로 한칸 쉬프트 시켜서 디코더 입력으로 만드는 일이다. 앞서 이런 과정을 간단히기 직접 코딩해서 확인했지만 이런 작업을 자동으로 처리해주는 클래스가 `DataCollatorForSeq2Seq`이다.\n",
    "\n",
    "우선 콜레이터를 만들기 위해서는 토크나이저와 모델을 넘겨야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b617fd4e-83d7-4b96-aee6-dd3a75c5fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011c673-dea9-4989-bb98-153a68b22b9e",
   "metadata": {},
   "source": [
    "앞서 만들어논 `tokenized_datasets`에서 샘플 두개를 조회하면 다음처럼 전체 결과는 사전으로 리턴되며 사전의 각 키 아래에 여러 샘플들의 값이 리스트로 들어있게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4095b10-346f-4c94-be51-8738d47ed1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[4014, 322, 3170, 147, 67, 23274, 3, 1],\n",
       "  [11783, 4412, 96, 6556, 709, 1632, 3, 1]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]],\n",
       " 'labels': [[6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n",
       "  [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 항목아래 샘플들이 리스트 형태로 묶여 반환된다.\n",
    "tokenized_datasets[\"train\"][1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84efe94-7a2f-42ec-ac0c-421405f95ccc",
   "metadata": {},
   "source": [
    "콜레이터에 샘플을 넘길 때는 개별 샘플이 사전으로 묶이는 형태가 되어야 되므로 아래처럼 한번 가공하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ba462e4-07bb-4de7-a049-951b599ac4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [4014, 322, 3170, 147, 67, 23274, 3, 1],\n",
       "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'labels': [6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1]},\n",
       " {'input_ids': [11783, 4412, 96, 6556, 709, 1632, 3, 1],\n",
       "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'labels': [9881, 18590, 3837, 70, 4341, 1086, 677, 35, 426, 2255, 3, 1]}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 콜레이터에는 샘플을 개별 {}로 넘겨야 됨\n",
    "[tokenized_datasets[\"train\"][i] for i in range(1, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c7536-c012-4388-a118-7a7d18f30e5f",
   "metadata": {},
   "source": [
    "위에 반환된 결과를 보면 각 샘플이 사전 {}으로 묶이고 샘플 하나에는 `input_ids`, `attention_mask`, ` labels`이 존재한다. 각 샘플을 리스트로 묶어서 콜레이터에게 전달하고 반환되는 값을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e7a57609-8fa4-4ec8-9949-b27f76e372e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# 콜레이터를 돌리면 알아서 패딩하고 쉬프트 시킨다.\n",
    "batch = data_collator(\n",
    "    [tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b0b004-c777-4041-8b5e-53b3062b0b01",
   "metadata": {},
   "source": [
    "반환된 `batch`의 키를 확인해보면 `decoder_input_ids`가 생긴것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e866df9c-ea46-42ec-b670-007300f2714c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc1a4c-91b2-4a67-a0a2-149d98784181",
   "metadata": {},
   "source": [
    "`batch`의 각 키에 어떤 값들이 들어있는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee2fe68c-97ff-44b5-a600-b2933ba52a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n",
       "        [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,    15,\n",
       "          1587,     3,     1],\n",
       "        [ 9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,  2255,\n",
       "             3,     1,  -100]]), 'decoder_input_ids': tensor([[    0,  6842,   404,   951,  5767, 15387,    27,   831,   800,  4378,\n",
       "            15,  1587,     3],\n",
       "        [    0,  9881, 18590,  3837,    70,  4341,  1086,   677,    35,   426,\n",
       "          2255,     3,     1]])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183b680-f156-4552-a060-44447843bed4",
   "metadata": {},
   "source": [
    "출력된 batch를 정리하면 아래처럼 된다.\n",
    "\n",
    "```python\n",
    "{\n",
    "    'input_ids': \n",
    "        tensor([[ 4014,   322,  3170,   147,    67, 23274,     3,     1],\n",
    "                [11783,  4412,    96,  6556,   709,  1632,     3,     1]]), \n",
    "    'attention_mask': \n",
    "        tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1]]), \n",
    "    'labels': \n",
    "        tensor(\n",
    "            [[ 6842, 404, 951, 5767, 15387, 27, 831, 800, 4378, 15, 1587, 3, 1],\n",
    "             [ 9881,18590,3837,70,4341,1086,677,35,426,2255,3,1,-100]]), \n",
    "    'decoder_input_ids': \n",
    "        tensor(\n",
    "            [[ 0,6842,404,951,5767,15387,    27,   831, 800,  4378, 15,  1587,     3],\n",
    "             [ 0,9881,18590,3837,70,4341,1086,677, 35,   426,2255,     3,     1]])\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "새로 생긴 `decoder_input_ids`는 앞에 0(<pad>)이 붙어 있는 것이 보이고 `label`에서 끝에 1이 사라져 `label`이 오른쪽으로 쉬프트된 것임을 알 수 있다. 그리고 또 두번째 샘플 `labels`에서 마지막에 -100 이 보인다. 이 값은 `label`이 패딩된 것을 나타내며 손실 함수값을 계산할 때 -100이 있는 위치는 손실을 계산하지 않게 된다. 이렇게 시퀀스 투 시퀀스 모델을 학습하기 위해 필요한 자잘한 작업을 콜레이터가 알아서 자동으로 처리한다.\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40676c-1393-423e-ad87-0c015f603bcb",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb743a-6e20-490c-8fa7-392c3a24e234",
   "metadata": {},
   "source": [
    "마지막으로 학습한 모델을 측정할 매트릭을 준비해야 한다. 번역 모델에서는 주로 BLEU 점수를 사용한다. BLEU 점수는 번역기가 생성한 문장이 레퍼런스(정답이라는 표현을 사용하지 않는 이유는 제대로 된 번역 문장이 오직 하나가 아니기 때문)문장과 얼마나 비슷한지 측정하는 점수라고 생각하면 된다. 단 같은 단어가 반복된다든지 레퍼런스 문장보다 너무 짧은 문장을 생성한다든지 하면 패널티를 부여 한다. 그렇기 때문에 레퍼런스 문장과 길이가 최대한 비슷하고 다양한 단어를 사용하면서 생성된 문장의 단어가 레퍼런스 단어에 많이 보여야 높은 점수를 얻게 된다.\n",
    "\n",
    "BLEU를 계산하기 위해 허깅페이스 [evaluate](https://huggingface.co/docs/evaluate/index) 라이브러리와 [sacrebleu](https://huggingface.co/spaces/evaluate-metric/sacrebleu)라이브러리를 제일 처음에 설치했었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4942a4-ddbe-47d3-b465-ac3c136c4cfc",
   "metadata": {},
   "source": [
    "`sacrebleu` 라이브러리는 BLEU 구현체에서 사실상 표준 라이브러리이며 각 모델이 다른 토크나이저를 쓰는 경우 이를 BPE로 통일 시켜 BLEU 점수를 계산한다고 한다. [참고링크](https://tech.kakaoenterprise.com/48)\n",
    "\n",
    "`evaluate`라이브러리로 이 `sacrebleu`를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00382b4a-a0b2-4ee8-8a18-9fda35053bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86f4f1-c1b4-4971-a928-b7108d2f952f",
   "metadata": {},
   "source": [
    "아래와 같은 예제가 있을 때 두 영어 문장을 번역기가 `predictions`처럼 번역했고 데이터 셋에 두 문장의 레퍼런스 번역이 `references`처럼 두개씩 있을 때 bleu점수를 계산해보면 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97bb5ea8-4599-435f-9197-72b119a37648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 100.00000000000004,\n",
       " 'counts': [21, 19, 17, 15],\n",
       " 'totals': [21, 19, 17, 15],\n",
       " 'precisions': [100.0, 100.0, 100.0, 100.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 21,\n",
       " 'ref_len': 21}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"저는 딥러닝을 좋아해요.\",\n",
    "    \"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n",
    "    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n",
    "     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62030f11-3ab1-4417-9e3e-095d06a0d3e1",
   "metadata": {},
   "source": [
    "첫 예에서는 `predictions`가 `references`의 두 문장 중 하나와 완전히 일치하므로 `score`가 100점이 나왔다. 하지만 약간 다른 식으로 번역을 한다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f392bab4-36af-4d6f-909e-bffcf62f7d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 25.28116160010779,\n",
       " 'counts': [14, 7, 4, 1],\n",
       " 'totals': [19, 17, 15, 13],\n",
       " 'precisions': [73.6842105263158,\n",
       "  41.1764705882353,\n",
       "  26.666666666666668,\n",
       "  7.6923076923076925],\n",
       " 'bp': 0.9000876262522591,\n",
       " 'sys_len': 19,\n",
       " 'ref_len': 21}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"저는 딥러닝을 좋아해요.\",\n",
    "    \"딥러닝 프레임워크가 잘 개발되었기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\"저는 딥러닝을 좋아해요.\", \"나는 딥러닝을 사랑해요.\"],\n",
    "    [\"요즘은 딥러닝 프레임워크가 잘 발달되어 있기 때문에 누구의 도움 없이도 기계 번역 시스템을 구축할 수 있습니다.\",\n",
    "     \"최근에는 딥러닝 프레임워크가 잘 개발되어 있기 때문에 다른 사람의 도움 없이도 기계 번역 시스템을 개발할 수 있습니다.\"]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a8740-b20f-4c94-85f9-81ccd7a4b913",
   "metadata": {},
   "source": [
    "점수가 떨어지는 것을 확인할 수 있다. 아래 함수는 모델의 예측과 레이블을 가지고 bleu를 계산하는 헬퍼 함수로 [트랜스포머 학습 코스 번역기 매트릭](https://huggingface.co/course/chapter7/4?fw=pt#metrics)에서 제공하는 코드를 그대로 복사 한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55c06b73-a43c-4925-816b-6bc692e23b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a8f57-668e-4f37-99dd-78a2ed390e5b",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "학습을 간단히 하기위해 허깅페이스에서 제공하는 `Seq2SeqTrainer`클래스를 사용한다. 학습 세부 조건은 `Seq2SeqTrainingArguments`를 사용하여 설정한다. 다음 코드로 학습에 필요한 세부 사항을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4813d2f-3a73-4a5b-a9e6-3175ce16400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"chkpt\",\n",
    "    learning_rate=0.0005,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"no\",\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to=\"none\" # Wandb 로그 끄기\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c5278-165d-4fd8-a4bf-7ce83eb1e693",
   "metadata": {},
   "source": [
    "이런 저런 자잘한 세팅을 해서 `training_args`를 만들고 `trainer`를 생성한다. 지금까지 준비한 `model`, `training_args`, `tokenized_datasets`, `data_collator`, `tokenizer`, `compute_metrics`를 넘기면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "792084d8-5bcc-443c-815a-132f8de2a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d42388-d7a4-4aca-b313-7d12fb118e06",
   "metadata": {},
   "source": [
    "이제 아래 코드로 드디어 학습을 할 수 있다! \n",
    "\n",
    "> [주의] 코랩에서 실행한다면 `per_device_train_batch_size`를 12정도로 줄여서 학습해야 하는데 학습 시간만 10시간이 넘게 걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f80da-3b68-4e33-99ac-064625b34542",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af6978-faac-4c4d-819d-84a17b2dc9cc",
   "metadata": {},
   "source": [
    "> [노트] 학습 과정에서 출력되는 로그 문장들이 너무 길어서 여기선 생략 되었음\n",
    "\n",
    "\n",
    "학습이 끝났으면 다음 셀을 실행해서 결과를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a051dbc-242e-49c3-ab4e-a44f3d348b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207a04a-18c9-4fdd-b0c6-003bedec3802",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437f4ec-9d7d-4584-abd0-629b1420f67a",
   "metadata": {},
   "source": [
    "학습과 저장을 성공적으로 마쳤으면 다음 명령으로 모델을 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "81c07221-cee1-44bc-b8d5-7fcdea9c99f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./results/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"./results\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64128\n",
      "}\n",
      "\n",
      "loading weights file ./results/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./results.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"./results\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "\n",
    "model.cpu();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f202ca6f-6aee-4133-a5e0-bad58e297c95",
   "metadata": {},
   "source": [
    "로딩된 모델을 테스트하기 위해 다음 두 문장을 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ac332afb-3825-4a78-98f2-af4d6ed26faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\n",
    "    \"Because deep learning frameworks are well developed, in these days, machine translation system can be built without anyone's help.\",\n",
    "    \"This system was made by using HuggingFace's T5 model for a one day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58249688-20d7-4437-bd09-2ee943961dd7",
   "metadata": {},
   "source": [
    "모델이 입력하기위해 토크나이저로 토큰화 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e0bcd6f-b746-4c92-89e2-d86d6170318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/metamath/miniconda3/envs/torchflow/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\", \n",
    "                   padding=True, max_length=max_token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aae199-726d-406b-9e84-021d32b7ceb5",
   "metadata": {},
   "source": [
    "`inputs`를 확인해보면 `input_ids`와 `attention_mask`로 토큰화 된것을 알 수 있다. 첫번째 문장이 더 길기 때문에 두번째 문장의 마스크는 마지막에 0으로 패딩된 것도 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f01d595e-6315-4348-9ada-b86e9dea923f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 8127,  5859,  5789, 22309,     8,    69,   484,  6560,     4,    20,\n",
       "           572,  1258,     4,  9872, 46301,  1076,   147,    67,  3807,  1215,\n",
       "          3993,    17,     8,   787,     3,     1],\n",
       "        [  465,  1076,    62,   565,    81,  1676,   992, 60049,  1044, 17400,\n",
       "            17,     8,   745,   466,  3900,    40,    16,   165,   688,     1,\n",
       "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0]])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a6e83-800c-4608-85ae-2203bc43e1ec",
   "metadata": {},
   "source": [
    "`model.generate()`에 입력을 넣고 출력을 생성한다. 이때 빔서치를 하기 위해 `num_beams=5`로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "00bee52e-d7b2-4ac5-ab45-74f2a67480ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koreans = model.generate(\n",
    "    **inputs,\n",
    "    max_length=max_token_length,\n",
    "    num_beams=5,\n",
    ")\n",
    "\n",
    "koreans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a140d93-3a37-4494-98fb-5486881f5bf8",
   "metadata": {},
   "source": [
    "생성된 결과를 디코딩해보면 다음처럼 나쁘지 않게 번역되는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c91f74e7-3167-449d-b7c3-f25edf03b514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> 딥러닝 틀이 잘 개발되기 때문에 요즘은 누군가의 도움 없이 기계번역 시스템을 구축할 수 있다.</s>',\n",
       " '<pad> 이 시스템은 HuggingFace의 T5 모델을 하루 동안 사용해 만든 시스템입니다.</s><pad>']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ \n",
    "    tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(korean)) for korean in koreans\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f518114-2ef2-4b57-85b5-3d1c58621afa",
   "metadata": {},
   "source": [
    "마지막으로 테스트 셋에 대해서 몇개 문장을 가져와 번역해보자. 만들어 놓은 `tokenized_datasets`과 `data_collator`를 `pytorch` `DataLoader`에 그대로 전달해서 데이터 로더를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0109830b-a60c-4f4b-bea3-379fa257340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2d02b-58e9-436c-8d7c-8a87e42b6e1d",
   "metadata": {},
   "source": [
    "이터레이터로 만들어 한 미니 배치만 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8757ccd1-6c5b-4b8c-a9e4-c8033e98585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_iter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65244048-fa29-4413-8eea-fef1e68cff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(test_dataloader_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39f299-8601-4edc-9f8c-7d35480321d5",
   "metadata": {},
   "source": [
    "콜레이터에 의해 반환된 미니 배치에는 다음처럼 `labels`, `decoder_input_ids` 따위도 가지고 있으므로 모델에 입력하기 위해 `input_ids`, `attention_mask`만 남긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9490c304-c29f-4080-a784-3f2ce1c059dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "93dbf9f2-3414-4a03-b0e8-15a673d86923",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = { key: test_batch[key] for key in ('input_ids', 'attention_mask') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "73197472-c0b3-42f3-8419-37246b654dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "koreans = model.generate(\n",
    "    **test_input,\n",
    "    max_length=max_token_length,\n",
    "    num_beams=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ea8e8-1bc2-4b99-84a7-802c20979418",
   "metadata": {},
   "source": [
    "이제 입력문장, 정답 그리고 생성된 문장을 비교하기 위해 우선 `test_batch.labels`에 -100으로 인코딩된 부분을 패딩 코튼으로 교체 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4033df5c-2d83-4ebb-8e38-0ef28a629f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =  np.where(test_batch.labels != -100, test_batch.labels, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6aaefdb1-602c-49dc-8485-8c1f4ab5310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sents = tokenizer.batch_decode(test_batch.input_ids, skip_special_tokens=True)[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6af6c308-b9b3-46ce-b321-b4c9ac9f450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = tokenizer.batch_decode(labels, skip_special_tokens=True)[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "64325b75-cae8-4a0b-96ab-5d3c99c58832",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tokenizer.batch_decode( koreans, skip_special_tokens=True )[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "df7480f2-be80-4d2a-a149-d618104dc2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English   : Yes, I'll see you at the parking lot at 3 p.m.\n",
      "Reference : 네, 오후 3시에 주차장에서 뵙죠.\n",
      "Translated: 네, 오후 3시에 주차장에서 뵙겠습니다.\n",
      "\n",
      "\n",
      "English   : I'm happy to see Jessica Huh take over my role.\n",
      "Reference : Jessica Huh가 제 역할을 인계받게 되어서 저는 기니다.\n",
      "Translated: 제시카 허가 제 역할을 맡아서 기뻐요.\n",
      "\n",
      "\n",
      "English   : I agree with you that she is qualified for the position.\n",
      "Reference : 저도 부장님 의견대로 그녀가 이 자리의 적임자라고 생각합니다.\n",
      "Translated: 나는 그녀가 이 직책에 자질이 있다고 당신과 동의합니다.\n",
      "\n",
      "\n",
      "English   : Nick, I was told that your department will be divided into two.\n",
      "Reference : Nick, 당신 부서가 둘로 나뉜다면서요?\n",
      "Translated: 닉, 당신의 부서가 두 가지로 나뉘게 될 거라고 들었습니다.\n",
      "\n",
      "\n",
      "English   : Yes, all staff involved in online advertising will have their own office space located on the 2nd floor.\n",
      "Reference : 네, 다음 달 초부터 온라인 광고와 관련된 직원들은 2층에 위치한 개별 사무실 공간을 사용한다고 합니다.\n",
      "Translated: 네, 온라인 광고에 관련된 모든 직원들은 2층에 각자의 사무실 공간이 위치해 있습니다.\n",
      "\n",
      "\n",
      "English   : What happens to the remaining staff?\n",
      "Reference : 남은 직원들은 어떻게 되나요?\n",
      "Translated: 남은 스태프에게 무슨 일이 일어났나요?\n",
      "\n",
      "\n",
      "English   : The remaining staff will stay in the current space on the 3rd floor, and the division will continue to be called the advertising department.\n",
      "Reference : 남은 직원들은 3층 현재 자리에 남을 것이고, 부서는 계속 광고 부서로 불린다고 하네요.\n",
      "Translated: 나머지 직원들은 3층 현 공간에 머물게 되며, 이 부서는 계속 광고부서로 불리게 된다.\n",
      "\n",
      "\n",
      "English   : I have a question about the year-end tax adjustment.\n",
      "Reference : 이번 연말 정산 관련해서 질문이 있어요.\n",
      "Translated: 저는 연말정산에 대해 궁금한 점이 있습니다.\n",
      "\n",
      "\n",
      "English   : Is there any problem?\n",
      "Reference : 무슨 문제라도 있으신가요?\n",
      "Translated: 혹시 문제가 있나요?\n",
      "\n",
      "\n",
      "English   : I am registering my dependent this time, so do I need to submit any particular documents?\n",
      "Reference : 제가 이번에 부양가족을 등록하려고 하는데, 따로 제출해야 하는 서류가 있나요?\n",
      "Translated: 이번에 내 국적 등록을 하고 있으니 특별히 서류 제출을 해야 하나요?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in zip(eng_sents, references, preds):\n",
    "    print('English   :', s[0])\n",
    "    print('Reference :', s[1])\n",
    "    print('Translated:', s[2])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4285f4-01ba-49d0-b56c-0939dda0f0d9",
   "metadata": {},
   "source": [
    "두 시간동안 대충 1 에폭만 학습한 것치고는 꽤 그럴듯 하게 번역을 하는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6525baa3-5e94-48de-9520-3716967eedab",
   "metadata": {},
   "source": [
    "## 마무리\n",
    "\n",
    "이상으로 영어-한국어 번역기를 처음부터 학습시키는 방법을 정리했다. 이 글을 글쓴이가 의도한대로 빠르게 읽고 이해하기 위해서는 트랜스포머에 대한 이해가 선행되야 한다. 트랜스포머에 대한 자세한 설명은 [진짜로 주석달린 트랜스포머](https://metamath1.github.io/2021/11/11/transformer.html)를 참고하자. 하지만 트랜스포머나 스퀀스 투 시퀀스 모델에 대해 잘 모른다 하더라도 한국어 번역기를 만들고자 할때 느끼는 막막함은 어느정도 해소할 수 있으리라 생각한다.\n",
    "\n",
    "이 글을 읽고 코드를 실행해보고 나서 `DataCollatorForSeq2Seq`나 `Seq2SeqTrainer` 를 쓰지 않고 직접 이 부분을 만들어서 모델을 학습 시켜본다면 트랜스포머를 이용한 번역 작업기 만들기를 훨씬 더 상세히 이해할 수 있을 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72abf57f-387b-4f5d-8e14-0742d3f73f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa92520-bdc5-4a94-ab1c-b0387e6cebfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9867695-a587-4710-ba1c-d3656b804b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
