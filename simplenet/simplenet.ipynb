{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 150줄로 된 간단한 네트워크Simple network with 150 lines\n",
    "\n",
    "metamath1/ml-simple-works 저장소에서 바로 보고 있다면 예쁘게 랜더링해서 보기 : http://nbviewer.jupyter.org/github/metamath1/ml-simple-works/blob/master/simplenet/simplenet.ipynb\n",
    "<hr/>\n",
    "\n",
    "여기에서는 아주 간단한 네트워크를 직접 구현해 보는 것을 목표로 한다. \n",
    "Nielsen문서<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">[1]</a>나 다른 문서들을 보면서 이론을 이해했으면 그 문서에서 제공하는 코드를 보게 되는데 아무래도 각 층을 객체지향적으로 구현하다보니 소스가 좀 복잡하다고 느껴진다. 특히 역전파같은 경우는 객체지향적으로 구현하기에 매우 적합한 구조라 생각하지만 모르는 입장에서는 막상 구현된 결과를 보면 조각조각 쪼개져 있어서 전체적인 그림을 그리기 힘들다는 단점이 있다. 그런 이유로 객체지향 구현보다 더 간단하지만(적어도 내가 생각하기에는) 비슷한 성능을 내는 코드를 처음부터 구현하면서 간단한 뉴럴네트워크에 대한 이해도를 높이려고 하였다.<br/>\n",
    "따라서 코드는 절차지향적으로 작성되었으며 Network을 정의하는 부분만 Class를 사용하였다. 나머지는 모두 데이터가 입력되고 웨이트와 곱해지고 하는 네트워크의 절차적 흐름을 최대한 가독성 높게 구현하려고 하였다.\n",
    "\n",
    "## 1. 구현 요구 사항\n",
    "<hr/>\n",
    "\n",
    "- 당연히 Numpy로만 구현\n",
    "\n",
    "- 최대한 간단하게 150줄 내외로 구현해서 언제나 봐도 30분안에 이해되도록\n",
    "\n",
    "- Nielsen 문서의 수식 인덱스와 구현된 파이썬 인덱스를 완전히 일치시킴. 즉 다음 기본 식 4개의 인덱스와 파이썬 프로그램 배열의 인덱스를 완전히 일치시켜서 프로그램 하여 가독성을 최대화 함.\n",
    "\n",
    "$$\\delta^{L}_{j}= \\frac{\\partial C}{\\partial a^{L}_{j}} \\sigma^{'}(z^{L}_{j})$$\n",
    "\n",
    "$$\\delta^{l}_{j} = \\sum_{k} w^{l+1}_{kj} \\delta^{l+1}_{k}   \\sigma^{'}(z^{k}_{j}) $$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{l}_{j}} = \\delta^{l}_{j}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{l}_{jk}} = a^{l-1}_{k} \\delta^{l}_{j}$$\n",
    "\n",
    "- 완전 연결층만 구현\n",
    "\n",
    "- 코스트 함수 : Mean Square Error, Binary Cross Entropy, Categorical Cross Entropy\n",
    "\n",
    "- 활성 함수 : Sigmoid, Relu, Softmax\n",
    "\n",
    "- 기타 구현 : Regularization, Dropout, Batch Normalization\\*\n",
    "\n",
    "- 테스트데이터 : MNIST\n",
    "\n",
    "\\* 은 선택사항\n",
    "\n",
    "\n",
    "## 2. 구현 세부 사항\n",
    "<hr/>\n",
    "\n",
    "구현에 있어서 가장 어려운 부분은 \n",
    "\n",
    "$$\\delta^{L}_{j}= \\frac{\\partial C}{\\partial a^{L}_{j}} \\sigma^{'}(z^{L}_{j})$$\n",
    "\n",
    "을 구현하는 부분인데 왜냐하면 코스트 함수와 출력층의 활성화 함수를 직접 미분해서 코딩해야하기 때문이다. 보통은 $\\boldsymbol{\\delta}^{L}$을 구하기 위해 위 식을 그대로 코딩하지는 않고 위 식이 계산된 결과를 코딩하게 된다. 수치 안정성이나 코딩의 양이 적어 훨씬 효율적이기 때문이다. 구현에 사용되는 sigmoid, softmax와 mean square error, cross entropy함수를 적당히 잘 조합하면 $\\boldsymbol{\\delta}^{L} = \\mathbf{a}-\\mathbf{y}$가 되어 간단하게 코딩할 수 있는데 여기서는 학습의 목적으로 코스트함수와 활성화 함수를 직접 다 미분해서 구현하였다. 때문에 원하는대로 출력층 활성화 함수와 코스트함수를 마음대로 조합해가며 실험할 수 있다. 예를 들어 sigmoid-cross entropy 같은 조합도 가능한데 이렇게 하면 코스트함수가 sigmoid 출력의 특성을 잘 반영하지 못하기 때문에 학습이 안되는 결과를 확인할 수 있다.\n",
    "\n",
    "전체적인 구성은 다음과 같이 활성함수와 코스트함수를 함수값 계산, 미분값 계산을 하는 두 부분으로 구현하고 각각을 'f', 'df'를 키로 하는 사전으로 묶어 둔다.\n",
    "\n",
    "```python\n",
    "#--------------------------------------------------------------------\n",
    "# activation functions\n",
    "# 활성함수의 함수값을 계산하는 함수와 미분값을 계산하는 함수를 한쌍으로 정의\n",
    "# 'f'와 'df' 키워드로 묶음\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    z : (C, N), C:클래스 수, N:샘플 수 이하 동일\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    " \n",
    "def sigmoid_prime(z, y):\n",
    "    \"\"\"\n",
    "    z : (C, N)\n",
    "    y : dummy\n",
    "    \n",
    "    [참고]\n",
    "    여기서는 정답 벡터 y가 필요없지만 softmax_prime_with_crossent과 인터페이스를 맞추기 위해\n",
    "    더미로 y를 넘겨 받음 None로 넘겨주면 됨.\n",
    "    \"\"\" \n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "    \n",
    "Sigmoid = { 'f': sigmoid, 'df': sigmoid_prime }\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Cost function\n",
    "# 목적함수의 함수값을 계산하는 함수와 미분값을 계산하는 함수를 한쌍으로 정의\n",
    "# 'f'와 'df' 키워드로 묶음\n",
    "def mse_cost(a, y):\n",
    "    \"\"\"\n",
    "    a : (C, N)\n",
    "    y : col major로 인코딩된 one hot (C, N)\n",
    "    \"\"\"\n",
    "    return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "def mse_cost_prime(a, y):\n",
    "    \"\"\"\n",
    "    a : (C, N)\n",
    "    y : col major로 인코딩된 one hot (C, N)\n",
    "    \"\"\"\n",
    "    return (a-y)\n",
    "\n",
    "Mse = {'f':mse_cost, 'df':mse_cost_prime}\n",
    "```\n",
    "\n",
    "이렇게 구현해두면 필요한 것만 골라서 조합해서 네트워크를 구성할 수 있게 된다. 프로그램의 구조는 위 코드가 핵심이며 다른건 별게 없다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cross Entropy Cost and Softmax \n",
    "\n",
    "다른 부분은 주석이 자세하게 달려 있어서 문서로는 cross entropy를 코스트함수로 쓰고 출력층 활성화함수를 softmax로 할 때 미분 관련해서만 남긴다. 네트워크를 학습시키기 위한 목적함수(또는 코스트,  로스함수)는 스칼라함수이고 sigmoid, tanh 함수같은 경우 역시 스칼라 함수이므로 둘다 미분을 해서 곱하기에 무리가 없다.  $\\boldsymbol{\\delta}^{L}$을 구하기 위한 다음 식을 보면\n",
    "\n",
    "$$\\delta^{L}_{cn}= \\frac{\\partial C}{\\partial a^{L}_{cn}} \\frac{d \\sigma(z^{L}_{cn})}{d z^{L}_{cn}}$$\n",
    "\n",
    "인데 이 식에서 $c$는 $L$층의 뉴런 인덱스이고 $n$는 샘플의 인덱스이다. 그렇게 보면 위 식의 $\\delta^{L}_{cn}$은 행렬, $\\dfrac{\\partial C}{\\partial a^{L}_{cn}}$도 행렬이고, sigmoid, tanh같은 함수를 $\\sigma$로 썼다면 $\\dfrac{d \\sigma(z^{L}_{cn})}{d z^{L}_{cn}}$ 역시 행렬이 되어 두 행렬 $\\dfrac{\\partial C}{\\partial \\mathbf{a}^{L}}$와 $\\dfrac{d \\sigma(\\mathbf{z}^{L})}{ d \\mathbf{z}^{L}}$를 element-wise하게 곱하면 된다. <br/>\n",
    "하지만 softmax함수는 $\\mathbb{R}^{c} \\to \\mathbb{R}^{c}$인 벡터함수로 입력벡터 $\\mathbf{z}^{L}$로의 미분이 자코비안 행렬이 된다. 샘플의 개수가 $N$개라면 이런 행렬이 $N$개가 있는 $N \\times C \\times C$인 텐서(정확히는 3차원 어레이)가 된다. 그래서 위 식을 처리할때 약간 주의를 해야한다. 단계적으로 정리를 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Cost 함수 미분\n",
    "\n",
    "Cross Entropy 코스트의 정의는 아래와 같다.\n",
    "\n",
    "$$\\text{C}= - \\sum_{C} y_{c} \\log a_{c}\\equiv - \\log a_{y} $$\n",
    "\n",
    "이 식에서 $C$는 클래스의 수를 나타낸다. $y_c$는 크기 $C \\times 1$ 원핫벡터인 $\\mathbf{y}$의 $c$번째 요소를 나타내고 마지막 $a_{y}$는 출력 $a$중에 $y$번째 값을 나타낸다. 여기서 $y$는 원핫인코딩 되지 않은 0, 1, 2, 3, 4, 5, 6, 7, 8, 9의 값을 가지는 정답 숫자이다. N개의 샘플에 대한 네트워크의 마지막 층에서 가중합과 softmax에 의한 출력은 각각 다음과 같다. 인덱스 순서는 앞의 인덱스가 클래스 번호, 뒤에 인덱스가 샘플번호이다.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "z_{11} & z_{12} & \\cdots & z_{1N} \\\\[4pt]\n",
    "z_{21} & z_{22} & \\cdots & z_{2N} \\\\[4pt]\n",
    "z_{31} & z_{32} & \\cdots & z_{3N} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "z_{C1} & z_{C2} & \\cdots & z_{CN} \n",
    "\\end{bmatrix}\n",
    "\\xrightarrow{\\text{softmax}}\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1N} \\\\[4pt]\n",
    "a_{21} & a_{22} & \\cdots & a_{2N} \\\\[4pt]\n",
    "a_{31} & a_{32} & \\cdots & a_{3N} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{C1} & a_{C2} & \\cdots & a_{CN} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "그러면 이 코스트에 대한 미분은 다음과 같다.\n",
    "\n",
    "$$\\frac{\\partial \\, \\text{C}}{\\partial \\, \\mathbf{a}} = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial \\, \\text{C}}{\\partial a_{11}} & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{12}}& \\cdots & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{1N}} \\\\[4pt]\n",
    "\\dfrac{\\partial \\, \\text{C}}{\\partial a_{21}} & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{22}}& \\cdots & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{2N}} \\\\[4pt]\n",
    "\\dfrac{\\partial \\, \\text{C}}{\\partial a_{31}} & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{32}}& \\cdots & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{3N}} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial \\, \\text{C}}{\\partial a_{C1}} & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{C2}}& \\cdots & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{CN}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "그런데 코스트함수의 정의상 정답자리를 제외한 대부분의 위치는 0 이다. 예를 들어 $y_{21}$, $y_{32}$, $y_{CN}$등이 정답자리(바꿔 말하면 1번 샘플은 2, 2번 샘플은 3, ...)라고 하면\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\, \\text{C}}{\\partial \\, \\mathbf{a}} = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial \\, \\text{C}}{\\partial a_{11}} & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{12}}& \\cdots & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{1N}} \\\\[4pt]\n",
    "\\dfrac{\\partial \\, \\text{C}}{\\partial a_{21}} & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{22}}& \\cdots & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{2N}} \\\\[4pt]\n",
    "\\dfrac{\\partial \\, \\text{C}}{\\partial a_{31}} & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{32}}& \\cdots & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{3N}} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial \\, \\text{C}}{\\partial a_{C1}} & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{C2}}& \\cdots & \\dfrac{\\partial \\, \\text{C}}{\\partial a_{CN}} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & \\cdots & 0 \\\\[4pt]\n",
    "-\\dfrac{1}{a_{21}} & 0 & \\cdots & 0 \\\\[4pt]\n",
    "0 & -\\dfrac{1}{a_{32}} & \\cdots & 0 \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & -\\dfrac{1}{a_{CN}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "와 같이 미분한 행렬이 완성된다. python 코드로는 다음처럼 구현할 수 있다. 이때 출력 $a$가 softmax로부터의 출력일 경우 $a$가 거의 0일 수 있기때문에 오버플로 방지대책을 세워준다.\n",
    "\n",
    "```python\n",
    "def cross_entropy_cost_prime(a, y):\n",
    "    dC = np.zeros(a.shape)\n",
    "    a[np.where(a < np.finfo(np.float32).eps)] = np.finfo(np.float32).eps\n",
    "    dC[np.where(y ==1)] = -y[np.where(y==1)]/a[np.where(y==1)]\n",
    "    \n",
    "    return dC  #(C,N)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Softmax 함수 미분\n",
    "\n",
    "샘플 $\\mathbf{x}_{1}$에 대한 softmax함수의 미분은 다음과 같다.\n",
    "\n",
    "$$\\frac{\\partial \\, \\mathbf{a}_{\\mathbf{x}_{1}}}{\\partial \\, \\mathbf{z}_{\\mathbf{x}_{1}}} = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial \\, a_{11}}{\\partial z_{11}} & \\dfrac{\\partial \\, a_{11}}{\\partial z_{21}}& \\cdots & \\dfrac{\\partial \\, a_{11}}{\\partial z_{C1}} \\\\[4pt]\n",
    "\\dfrac{\\partial \\, a_{21}}{\\partial z_{11}} & \\dfrac{\\partial \\, a_{21}}{\\partial z_{21}}& \\cdots & \\dfrac{\\partial \\, a_{21}}{\\partial z_{C1}} \\\\[4pt]\n",
    "\\dfrac{\\partial \\, a_{31}}{\\partial z_{11}} & \\dfrac{\\partial \\, a_{31}}{\\partial z_{21}}& \\cdots & \\dfrac{\\partial \\, a_{31}}{\\partial z_{C1}} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial \\, a_{C1}}{\\partial z_{11}} & \\dfrac{\\partial \\, a_{C1}}{\\partial z_{21}}& \\cdots & \\dfrac{\\partial \\, a_{C1}}{\\partial z_{C1}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "실제 softmax의 미분은 다음과 같으므로 [https://www.facebook.com/groups/TensorFlowKR/permalink/502663916741338/]\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\, \\text{softmax}(z_{j})}{\\partial \\, z_{i}}=\n",
    "\\begin{cases}\n",
    "\\text{softmax}(z_{j})(1-\\text{softmax}(z_{i})), & \\mbox{if } j=i \\\\\n",
    "-\\text{softmax}(z_{j})\\text{softmax}(z_{i}) , & \\mbox{if } j \\neq i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "샘플 $\\mathbf{x}_{1}$에 대한 softmax함수의 미분이 아래처럼 완성된다.\n",
    "\n",
    "$$\\frac{\\partial \\, \\mathbf{a}_{\\mathbf{x}_{1}}}{\\partial \\, \\mathbf{z}_{\\mathbf{x}_{1}}} = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial \\, a_{11}}{\\partial z_{11}} & \\dfrac{\\partial \\, a_{11}}{\\partial z_{21}}& \\cdots & \\dfrac{\\partial \\, a_{11}}{\\partial z_{C1}} \\\\[4pt]\n",
    "\\dfrac{\\partial \\, a_{21}}{\\partial z_{11}} & \\dfrac{\\partial \\, a_{21}}{\\partial z_{21}}& \\cdots & \\dfrac{\\partial \\, a_{21}}{\\partial z_{C1}} \\\\[4pt]\n",
    "\\dfrac{\\partial \\, a_{31}}{\\partial z_{11}} & \\dfrac{\\partial \\, a_{31}}{\\partial z_{21}}& \\cdots & \\dfrac{\\partial \\, a_{31}}{\\partial z_{C1}} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial \\, a_{C1}}{\\partial z_{11}} & \\dfrac{\\partial \\, a_{C1}}{\\partial z_{21}}& \\cdots & \\dfrac{\\partial \\, a_{C1}}{\\partial z_{C1}} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11}(1-a_{11}) & -a_{11}a_{21} & \\cdots & -a_{11}a_{C1} \\\\[4pt]\n",
    "-a_{21}a_{11} & a_{21}(1-a_{21}) & \\cdots & -a_{21}a_{C1} \\\\[4pt]\n",
    "-a_{31}a_{11} & -a_{31}a_{21} & \\cdots & -a_{31}a_{C1} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-a_{C1}a_{11} & -a_{C1}a_{21} & \\cdots & a_{C1}(1-a_{C1})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "이렇게 샘플 $\\mathbf{x}_{n}$에 대해 하나의 미분 행렬이 주어지므로 최종 결과는 $N \\times C \\times C$인 텐서가 된다.\n",
    "\n",
    "```python\n",
    "def softmax_prime(z, y):\n",
    "    C, N = z.shape\n",
    "    di = np.diag_indices(C)\n",
    "    \n",
    "    a = softmax(z) # a : (C,N)\n",
    "    da = -np.einsum('ij,jk->jik', a, a.T)  #da :(N,C,C)\n",
    "    da[:,di[0],di[1]] = (a*(1-a)).T\n",
    "    \n",
    "    return da\n",
    "```\n",
    "\n",
    "이제 코스트함수의 미분 softmax함수의 미분이 끝났으므로 이 둘을 곱해서 $\\boldsymbol{\\delta}^{L}$을 만들어야 한다.  $\\boldsymbol{\\delta}^{L}$의 크기는 샘플 1개당 $C \\times 1$짜리 벡터가 모여있는 것이므로 $C \\times N$이 된다. 샘플 $\\mathbf{x}_{1}$에 대한 $\\boldsymbol{\\delta}_{\\mathbf{x}_{1}}^{L}$은 다음처럼 계산할 수 있다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left( \\boldsymbol{\\delta}_{\\mathbf{x}_{1}}^{L} \\right)^{\\text{T}} &= \n",
    "\\begin{bmatrix}\n",
    "0 & -\\dfrac{1}{a_{21}} & 0 & \\cdots & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\frac{\\partial \\, \\mathbf{a}_{\\mathbf{x}_{1}}}{\\partial \\, \\mathbf{z}_{\\mathbf{x}_{1}}} \\\\[5pt]\n",
    "&= \n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "0 & -\\dfrac{1}{a_{21}} & 0 & \\cdots & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "}_{ \\dfrac{\\partial C}{\\partial a^{L}_{c1}} }\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "a_{11}(1-a_{11}) & -a_{11}a_{21} & \\cdots & -a_{11}a_{C1} \\\\[4pt]\n",
    "-a_{21}a_{11} & a_{21}(1-a_{21}) & \\cdots & -a_{21}a_{C1} \\\\[4pt]\n",
    "-a_{31}a_{11} & -a_{31}a_{21} & \\cdots & -a_{31}a_{C1} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-a_{C1}a_{11} & -a_{C1}a_{21} & \\cdots & a_{C1}(1-a_{C1})\n",
    "\\end{bmatrix}\n",
    "}_{ \\dfrac{\\partial \\, \\mathbf{a}_{\\mathbf{x}_{1}}}{\\partial \\, \\mathbf{z}_{\\mathbf{x}_{1}}} }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "위 식의 첫항은 코스트함수 $\\text{C}$를 $a_{c1}$로 미분한 것으로 $\\dfrac{\\partial \\, \\text{C}}{\\partial \\, \\mathbf{a}}$의 첫번째 열이 된다. 이런 계산을 샘플 $\\mathbf{x}$에 대해서 $N$번 반복하면 $C \\times N$의 $\\boldsymbol{\\delta}^{L}$이 만들어 진다. numpy의 einsum함수를 사용해서 다음처럼 구현할 수 있다.\n",
    "\n",
    "```python\n",
    "#코스트 미분과 출력층의 미분\n",
    "dC , da = self.cost['df'](self.A[l], Y) , self.activates[l]['df'](self.Z[l], Y)\n",
    "\n",
    "#둘을 곱한다.\n",
    "delta = np.einsum('ij,jik->jk', dC, da).T\n",
    "```\n",
    "\n",
    "그런데 출력층의 활성화 함수가 스칼라 함수였다면 간단하게 아래처럼 할 수 있다.\n",
    "\n",
    "```python\n",
    "delta = dC * da #dC/da, da/az 둘 모양이 같으면 그냥 Hadamard 곱, 결과:(C,N)\n",
    "```\n",
    "\n",
    "이렇게 간단히 곱하기 위해 코스트의 미분과 softmax의 미분을 각각 벡터와 행렬형태로 줄여서 구현하는 방법이 2.1.3절에 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 축약버전\n",
    "\n",
    "축약 버전은 결국 지금까지 계산한 결과에 실제 곱해지는 성분만 뽑아서 정리해서 곱하는 방법인데 TF-KR에 내가 한 질문글을 바탕으로 작성하였다. [http://fbsight.com/t/topic/117240/10] 실제 답변을 주신 Kyung Mo Kweon[https://www.facebook.com/kkweon] 의 답변을 보고 코딩함.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left( \\boldsymbol{\\delta}_{\\mathbf{x}_{1}}^{L} \\right)^{\\text{T}} &= \n",
    "\\begin{bmatrix}\n",
    "0 & -\\dfrac{1}{a_{21}} & 0 & \\cdots & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\frac{\\partial \\, \\mathbf{a}_{\\mathbf{x}_{1}}}{\\partial \\, \\mathbf{z}_{\\mathbf{x}_{1}}} \\\\[5pt]\n",
    "&= \n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "0 & -\\dfrac{1}{a_{21}} & 0 & \\cdots & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "}_{ \\dfrac{\\partial C}{\\partial a^{L}_{c1}} }\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "a_{11}(1-a_{11}) & -a_{11}a_{21} & \\cdots & -a_{11}a_{C1} \\\\[4pt]\n",
    "-a_{21}a_{11} & a_{21}(1-a_{21}) & \\cdots & -a_{21}a_{C1} \\\\[4pt]\n",
    "-a_{31}a_{11} & -a_{31}a_{21} & \\cdots & -a_{31}a_{C1} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-a_{C1}a_{11} & -a_{C1}a_{21} & \\cdots & a_{C1}(1-a_{C1})\n",
    "\\end{bmatrix}\n",
    "}_{ \\dfrac{\\partial \\, \\mathbf{a}_{\\mathbf{x}_{1}}}{\\partial \\, \\mathbf{z}_{\\mathbf{x}_{1}}} }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "위 식에서 $\\boldsymbol{\\delta}_{\\mathbf{x}_{1}}^{L}$를 구하기 위해서는 결국 앞 항의 $-\\dfrac{1}{a_{21}}$를 뒤 항의 두번째 행 $\\left[ -a_{21}a_{11} \\,,\\, a_{21}(1-a_{21}) \\,,\\, \\cdots \\,,\\, -a_{21}a_{C1} \\right]$와 곱하면 된다. 나머지 샘플 $\\mathbf{x}$에 대해서도 마찬가지이므로 코스트 함수 미분 행렬에서 값이 있는 것만 뽑아서 벡터로 만든다. 그리고 softmax를 미분한 $N$개의 행렬에서 코스트 함수 미분 행렬에서 값이 있는 위치의 행만을 뽑아서 하나의 행렬로 만든다. 지금 예를 들어 정답자리를 $y_{21}$, $y_{32}$, $y_{CN}$등으로 가정하고 있기 때문에 이 경우는 아래처럼 벡터와 행렬이 구성된다.\n",
    "\n",
    "$$\n",
    "\\left( \\boldsymbol{\\delta}^{L} \\right)^{\\text{T}} = \n",
    "\\underbrace{\n",
    "\\begin{Bmatrix} -\\dfrac{1}{a_{21}} \\\\ -\\dfrac{1}{a_{32}} \\\\ \\vdots \\\\ -\\dfrac{1}{a_{CN}} \\end{Bmatrix}\n",
    "}_{N \\times 1}\n",
    "\\times\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial \\, a_{21}}{\\partial z_{11}} & \\dfrac{\\partial \\, a_{21}}{\\partial z_{21}}& \\cdots & \\dfrac{\\partial \\, a_{21}}{\\partial z_{C1}} \\\\[4pt]\n",
    "\\dfrac{\\partial \\, a_{32}}{\\partial z_{12}} & \\dfrac{\\partial \\, a_{32}}{\\partial z_{22}}& \\cdots & \\dfrac{\\partial \\, a_{32}}{\\partial z_{C2}} \\\\[4pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial \\, a_{CN}}{\\partial z_{1N}} & \\dfrac{\\partial \\, a_{CN}}{\\partial z_{2N}}& \\cdots & \\dfrac{\\partial \\, a_{CN}}{\\partial z_{CN}} \\\\\n",
    "\\end{bmatrix}\n",
    "}_{N \\times C}\n",
    "=\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "\\left( \\boldsymbol{\\delta}_{\\mathbf{x}_{1}}^{L} \\right)^{\\text{T}} \\\\[4pt]\n",
    "\\left( \\boldsymbol{\\delta}_{\\mathbf{x}_{2}}^{L} \\right)^{\\text{T}}  \\\\[4pt]\n",
    "\\vdots \\\\\n",
    "\\left( \\boldsymbol{\\delta}_{\\mathbf{x}_{N}}^{L} \\right)^{\\text{T}}  \\\\\n",
    "\\end{bmatrix}\n",
    "}_{N \\times C}\n",
    "$$\n",
    "\n",
    "여기서 곱하기는 python의 요소끼리 브로드캐스팅되는 곱을 의미한다.\n",
    "\n",
    "앞 항의 $N \\times 1$ 벡터를 전치된 상태로 구하는 코드는 다음과 같다. (위 수식에서 결과를 전치시켜야 $\\boldsymbol{\\delta}^{L}$이 되므로 바로 구하기 위해 전치된 상태로 각 미분 결과를 되될림)\n",
    "\n",
    "\n",
    "```python\n",
    "def cross_entropy_cost_prime_with_softmax(a, y):\n",
    "    y_label = np.where(y.T==1)[1]\n",
    "    a_t = a[y_label, np.arange(y.shape[1])].reshape(-1,1) #(N,1)\n",
    "    a_t[np.where(a_t < np.finfo(np.float32).eps)] = np.finfo(np.float32).eps\n",
    "    \n",
    "    dC = -1/a_t  # dC : (N,1)\n",
    "    return dC.T # (1,N)\n",
    "```\n",
    "\n",
    "뒷 항의 $N \\times C$ 행렬도 전치된 상태로 구하는 코드가 다음이다. 원래 softmax 미분하는데는 정답 $\\mathbf{y}$가 필요하지 않지만 여기서는 정답자리를 알아야 하기 때문에 정답을 인자로 받는다. (그래서 원래 softmax_prime함수에서는 $\\mathbf{y}$가 필요하지 않지만 인자를 통일시키기 위해 더미로 $\\mathbf{y}$를 입력 받았었다.)\n",
    "\n",
    "```python\n",
    "def softmax_prime_with_crossent(z, y):\n",
    "    C, N = z.shape\n",
    "    y_label = np.where(y.T==1)[1] #one-hot decoding , col major라서 전치후 where 적용\n",
    "    \n",
    "    a = softmax(z) # a : (C,N)\n",
    "    a_t = a[y_label, np.arange(N)].reshape(-1,1) #(N,1)\n",
    "    \n",
    "    #da/dz\n",
    "    da = -a_t * a.T   #dz : (N,1)*(N,C)=(N,C)\n",
    "    da[np.arange(N), y_label] = a_t.T * (1- a_t.T)\n",
    "    return da.T # (C,N)\n",
    "```\n",
    "\n",
    "그래서 이제 sigmoid, tanh, relu를 출력층 활성함수로 쓴 경우와 똑같이 아래 코드로 출력층 $\\boldsymbol{\\delta}^{L}$를 구할 수 있다.\n",
    "\n",
    "```python\n",
    "delta = dC * da #dC/da, da/dz 둘 모양이 같으면 그냥 Hadamard 곱, 결과:(C,N)\n",
    "```\n",
    "\n",
    "이런 식으로 동작하는 것을 확인 해보려면 아래 코드에서 주석을 바꿔주고 실험해보면 된다.\n",
    "\n",
    "```python\n",
    "Softmax = {'f':softmax, 'df':softmax_prime }\n",
    "#Softmax = {'f':softmax, 'df':softmax_prime_with_crossent }\n",
    "\n",
    "CrossEnt = {'f':cross_entropy_cost, 'df':cross_entropy_cost_prime}\n",
    "#CrossEnt = {'f':cross_entropy_cost, 'df':cross_entropy_cost_prime_with_softmax}\n",
    "```\n",
    "\n",
    "이렇게 해서 역전파의 기본 4개 수식을 있는 그대로 코드로 구현하게 되었다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 더 그냥 쉽게....\n",
    "\n",
    "2.1.2절 마지막 식의 곱이나 2.1.3절의 곱을 실제로 해보면 앞에서 이야기 했듯이 $\\boldsymbol{\\delta}^{L}_{\\mathbf{x}_{1}} = \\mathbf{a}^{L}_{\\mathbf{x}_{1}} - \\mathbf{y}_{\\mathbf{x}_{1}}$ \n",
    "라는 사실을 알 수 있다. 그래서 복잡하게 하지말고 softmax는 cross entropy하고 쓰니까 그냥 출력층 $\\boldsymbol{\\delta}^{L}$은 $ \\mathbf{a} - \\mathbf{y}$하면 된다. $ \\mathbf{a}$는 $C \\times N$이고 원핫인코딩된 $\\mathbf{y}$도 $C \\times N$ 이라 그냥 빼면 모든 것이 해결된다. 그래서 이 둘을 묶어서 Softmax classifier라 하기도 한다. 그냥 공부하려고 복잡하게 한 것임;;; Nielsen문서나 CS231n 숙제에도 보면 다 $ \\mathbf{a} - \\mathbf{y}$로 구현되어 있다. 이렇게 구현하면 효율적이기는 하나 위에 적은 역전파의 기본 4개 식중에서 1번식은 코드에 나타나지 않게 되어 수식을 보고 코드를 볼 때 수식이 생략된 부분때문에 코드를 해석하기 어려워진다는 단점이 있다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dropout 구현\n",
    "\n",
    "Dropout 구현을 위해서 다음 3가지 코드가 필요하다.\n",
    "\n",
    "[1] 포워드 패스시 계산된 출력 a에 대해 다음 코드를 적용하여 강제로 일정 비율 만큼 값을 0으로 만들어 버림\n",
    "\n",
    "\n",
    "```python\n",
    "z = np.dot(self.W[l], a)+self.B[l]\n",
    "a = self.activates[l]['f'](z)\n",
    "\n",
    "mask = np.random.binomial(n=1, p=1-self.p_dropouts[l], size=a.shape)\n",
    "self.mask.append(mask)\n",
    "a = mask*a\n",
    "```\n",
    "\n",
    "[2] 백워드 패스시 위 포워드 패스시 생성된 mask를 해당 층의 $\\delta$에 곱해준다.\n",
    "\n",
    "```python\n",
    "delta = delta*self.mask[l] # 포워드에서 저장해둔 드랍아웃 마스크를 곱한다.\n",
    "```\n",
    "\n",
    "[3] 마지막으로 테스트시 Dropout이 적용된 층의 출력에 (1-Dropout 비율)을 곱하여 출력 수준을 경감시킨다. \n",
    "\n",
    "```python\n",
    "a = self.activates[l]['f']( (1-self.p_dropouts[l-1])*np.dot(self.W[l], a)+self.B[l] )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:01 Train Acc. : 93.970000, Test Acc. : 94.240000\n",
      "EPOCH:02 Train Acc. : 95.386000, Test Acc. : 95.310000\n",
      "EPOCH:03 Train Acc. : 96.026000, Test Acc. : 95.930000\n",
      "EPOCH:04 Train Acc. : 96.422000, Test Acc. : 95.990000\n",
      "EPOCH:05 Train Acc. : 96.826000, Test Acc. : 96.140000\n",
      "EPOCH:06 Train Acc. : 96.830000, Test Acc. : 96.300000\n",
      "EPOCH:07 Train Acc. : 97.158000, Test Acc. : 96.760000\n",
      "EPOCH:08 Train Acc. : 97.216000, Test Acc. : 96.640000\n",
      "EPOCH:09 Train Acc. : 97.340000, Test Acc. : 96.570000\n",
      "EPOCH:10 Train Acc. : 97.514000, Test Acc. : 96.740000\n",
      "EPOCH:11 Train Acc. : 97.596000, Test Acc. : 96.830000\n",
      "EPOCH:12 Train Acc. : 97.534000, Test Acc. : 96.690000\n",
      "EPOCH:13 Train Acc. : 97.682000, Test Acc. : 96.810000\n",
      "EPOCH:14 Train Acc. : 97.754000, Test Acc. : 96.940000\n",
      "EPOCH:15 Train Acc. : 97.544000, Test Acc. : 96.580000\n",
      "EPOCH:16 Train Acc. : 97.746000, Test Acc. : 96.720000\n",
      "EPOCH:17 Train Acc. : 97.902000, Test Acc. : 96.990000\n",
      "EPOCH:18 Train Acc. : 97.926000, Test Acc. : 96.910000\n",
      "EPOCH:19 Train Acc. : 97.812000, Test Acc. : 96.960000\n",
      "EPOCH:20 Train Acc. : 98.048000, Test Acc. : 97.050000\n",
      "EPOCH:21 Train Acc. : 98.184000, Test Acc. : 97.450000\n",
      "EPOCH:22 Train Acc. : 98.056000, Test Acc. : 97.130000\n",
      "EPOCH:23 Train Acc. : 97.988000, Test Acc. : 97.200000\n",
      "EPOCH:24 Train Acc. : 98.110000, Test Acc. : 97.250000\n",
      "EPOCH:25 Train Acc. : 97.990000, Test Acc. : 97.090000\n",
      "EPOCH:26 Train Acc. : 97.976000, Test Acc. : 97.200000\n",
      "EPOCH:27 Train Acc. : 97.922000, Test Acc. : 96.870000\n",
      "EPOCH:28 Train Acc. : 97.882000, Test Acc. : 96.900000\n",
      "EPOCH:29 Train Acc. : 98.114000, Test Acc. : 97.250000\n",
      "EPOCH:30 Train Acc. : 98.176000, Test Acc. : 97.150000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCnoRkYBT0IuIBNz/AglVbU5YDctOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f149c411978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target  : [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1]\n",
      "Predict : [7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1]\n",
      "29\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAACSCAYAAABlhSBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14898e9f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCIrwA0ERfiAowg8E9f9qpE+21VoFiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f149c3eaef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# activation functions\n",
    "# 활성함수의 함수값을 계산하는 함수와 미분값을 계산하는 함수를 한쌍으로 정의\n",
    "# 'f'와 'df' 키워드로 묶음\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z : (C, N), C:클래스 수, N:샘플 수 이하 동일\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    " \n",
    "def sigmoid_prime(z, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z : (C, N)\n",
    "        y : dummy\n",
    "    \n",
    "    [참고]\n",
    "    여기서는 정답 벡터 y가 필요없지만 softmax_prime_with_crossent과 인터페이스를 맞추기 위해\n",
    "    더미로 y를 넘겨 받음 None로 넘겨주면 됨.\n",
    "    \"\"\" \n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "Sigmoid = { 'f': sigmoid, 'df': sigmoid_prime }\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z : (C, N)\n",
    "    \n",
    "    [참고]\n",
    "    max값을 빼서 언더플로 방지함.\n",
    "    \"\"\"\n",
    "    z = z - np.max(z, axis=0) #max함수는 elmenet wise하게 작동하지 않기 때문에 axis을 지정해야함.\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "def softmax_prime(z, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z : (C, N)\n",
    "        y : dummy\n",
    "    \n",
    "    [참고]\n",
    "    실제 softmax 함수 미분을 그대로 구현한것 \n",
    "    미분 결과는 샘플 하나당 (C,C) 행렬이 되어 전체 결과는 (N,C,C)의 텐서\n",
    "    보통은 softmax함수와 cross entropy를 함께 쓰므로 둘의 미분 곱해진 결과만 delta로 코딩을 함.\n",
    "    굳이 미분을 코딩한다 하더라도 둘의 곱이 간단히 Hadamard곱이 될 수 있도록\n",
    "    행렬로 정리해서 되돌리는 구현을 함. 그렇게 구현한 함수가 \n",
    "    아래 softmax_prime_with_crossent 임.\n",
    "    \n",
    "    여기서는 학습의 목적으로 미분 그대로를 다 구현했음.\n",
    "    \"\"\"\n",
    "    C, N = z.shape\n",
    "    di = np.diag_indices(C)\n",
    "    \n",
    "    a = softmax(z) # a : (C,N)\n",
    "    da = -np.einsum('ij,jk->jik', a, a.T)  #da :(N,C,C)\n",
    "    da[:,di[0],di[1]] = (a*(1-a)).T\n",
    "    \n",
    "    return da\n",
    "\n",
    "def softmax_prime_with_crossent(z, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z : (C, N)\n",
    "        y : col major로 인코딩된 one hot (C, N)\n",
    "    \n",
    "    [참고]\n",
    "    이 함수는 softmax_prime의 주석에서 설명한것처럼 결과를 행렬로 만들어 되돌리는 축약형\n",
    "    이 함수의 구현은 Cross Entropy 코스트와 함께 사용하도록 하기 위한 것으로 결과가 텐서\n",
    "    가 아니라 행렬이 되도록 구현하였음. 이렇게 구현하면 Sigmoid-Mse 를 쓴 구성과 마찬가지로 \n",
    "    출력층의 델터를 구하는 코드가 코스트의 미분 행렬(일반적으로 샘플이 여러개일때 코스트\n",
    "    의미분은 행렬이 됨) * 출력층 활성함수의 미분 행렬로 동일하게 됨. \n",
    "    실제 Cross Entropy-Softmax일 경우는 벡터*행렬이 됨.\n",
    "    일반적으로 구현하여 텐서를 되돌리는 softmax_prime함수를 쓰면 코스트의 미분과 출력층 \n",
    "    활성함수의 미분을 곱하는 곳에서 행렬 * 텐서가 되서 적당히 알아서 곱해야 해야함.\n",
    "    \"\"\"    \n",
    "    C, N = z.shape\n",
    "    y_label = np.where(y.T==1)[1] #one-hot decoding , col major라서 전치후 where 적용\n",
    "    \n",
    "    #정답 위치를 알아아됨\n",
    "    a = softmax(z) # a : (C,N)\n",
    "    a_t = a[y_label, np.arange(N)].reshape(-1,1) #(N,1)\n",
    "    \n",
    "    #da/dz\n",
    "    da = -a_t * a.T   #dz : (N,1)*(N,C)= (N,C)\n",
    "    da[np.arange(N), y_label] = a_t.T * (1- a_t.T)\n",
    "    return da.T # (C,N)\n",
    "    \n",
    "Softmax = {'f':softmax, 'df':softmax_prime }\n",
    "#Softmax = {'f':softmax, 'df':softmax_prime_with_crossent }\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z : (C, N)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z) #maximum 함수는 element wise로 작동을 함.\n",
    "\n",
    "def relu_prime(z, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z : (C, N)\n",
    "        y : dummy\n",
    "    \"\"\"\n",
    "    #da/dz\n",
    "    da = np.zeros(z.shape)\n",
    "    da[z >= 0] = 1\n",
    "    return da\n",
    "\n",
    "Relu = {'f':relu, 'df':relu_prime }\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Cost function\n",
    "# 목적함수의 함수값을 계산하는 함수와 미분값을 계산하는 함수를 한쌍으로 정의\n",
    "# 'f'와 'df' 키워드로 묶음\n",
    "def mse_cost(a, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a : (C, N)\n",
    "        y : col major로 인코딩된 one hot (C, N)\n",
    "    \"\"\"\n",
    "    return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "def mse_cost_prime(a, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a : (C, N)\n",
    "        y : col major로 인코딩된 one hot (C, N)\n",
    "    \"\"\"\n",
    "    return (a-y)\n",
    "\n",
    "Mse = {'f':mse_cost, 'df':mse_cost_prime}\n",
    "\n",
    "def bin_cross_entropy_cost(a, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a : (C, N)\n",
    "        y : col major로 인코딩된 one hot (C, N)\n",
    "    \n",
    "    [참고]\n",
    "    C = \\sum_{j} -[y_j*log(a_j)+(1-y_j)*log(1-a_y)]\n",
    "    j는 출력층 뉴런의 Index , mini-batch전체에 대해서 평균내는 부분은 없음\n",
    "    \"\"\"\n",
    "    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "def bin_cross_entropy_cost_prime(a, y):    \n",
    "    \"\"\" \n",
    "    Args:\n",
    "        a : (C, N)\n",
    "        y : col major로 인코딩된 one hot (C, N)\n",
    "    \n",
    "    [참고]\n",
    "    http://neuralnetworksanddeeplearning.com/chap3.html 의 Problem에 이 함수와 관련된 문제가 있음\n",
    "    분모의 a(네트워크의 활성값)이 1 근처로 가면 0으로 나누는 것과 같은 문제가 \n",
    "    발생할 가능성 있음.\n",
    "    return (a-y) / (a*(1-a)) 이렇게 하면 실제로 오버플로 일어남.\n",
    "    \n",
    "    출력증의 델타는 delta^{L} = dC/da * da/dz 인데 \n",
    "    결국 이 미분이 sigmoid의 미분과 곱해지면 분모가 약분되어 사라지면서 \n",
    "    a-y가 delta가 됨. 그래서 delta를 바로 코딩하는 방식을 주로 사용. \n",
    "    그럼 오버플로문제도 없어짐\n",
    "    이 코스트함수는 그렇게 sigmoid와 함께 사용하도록 의도적으로 설계된 함수임.\n",
    "    \n",
    "    여기서는 학습의 목적으로 미분을 그대로 다 구현했기 때문에 이 함수를 softmax와\n",
    "    조합해서 사용하는 것도 가능함.\n",
    "    https://www.reddit.com/r/MachineLearning/comments/39bo7k/can_softmax_be_used_with_cross_entropy/?st=j51xzboh&sh=1743ac2e\n",
    "    위 레딧 링크에서는 안된다하는데 딱히 안될건 없어보이는데... 결과도 괜찮게 나옴\n",
    "    \"\"\"\n",
    "    denom = (a*(1-a))\n",
    "    denom[np.where(denom<np.finfo(np.float32).eps)] = np.finfo(np.float32).eps\n",
    "    \n",
    "    return (a-y) / denom \n",
    "\n",
    "BinCrossEnt = {'f':bin_cross_entropy_cost, 'df':bin_cross_entropy_cost_prime}\n",
    "\n",
    "def cross_entropy_cost(a, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a : (C, N)\n",
    "        y : col major로 인코딩된 one hot (C, N)\n",
    "    \n",
    "    [참고]\n",
    "    C = \\sum_{k} y_k log(a_k)\n",
    "    \"\"\"\n",
    "    return np.sum(np.nan_to_num(-y*np.log(a)))\n",
    "\n",
    "def cross_entropy_cost_prime(a, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a : (C, N)\n",
    "        y : col major로 인코딩된 one hot (C, N)\n",
    "    \n",
    "    [참고]\n",
    "    미분해서 행렬로 그대로 되돌리는 함수\n",
    "    벡터로 되돌리는 함수는 cross_entropy_cost_prime_with_softmax 참조\n",
    "    \n",
    "    사실 bin_cross_entropy_cost_prime함수 주석에서 설명한것과 마찬가지 이유로\n",
    "    이 코스트 함수가 softmax와 함께 쓰이면 delta가 결국 또 a-y가 되므로 delta를\n",
    "    바로 코딩하는 것이 맞음.\n",
    "    하지만 역시나 학습의 목적으로 따로 다 구현해서 조합해서 쓰는 식으로 했음.\n",
    "    \"\"\"\n",
    "    dC = np.zeros(a.shape)\n",
    "    a[np.where(a<np.finfo(np.float32).eps)] = np.finfo(np.float32).eps\n",
    "    dC[np.where(y ==1)] = -y[np.where(y==1)]/a[np.where(y==1)]\n",
    "    \n",
    "    return dC  #(C,N)\n",
    "    \n",
    "def cross_entropy_cost_prime_with_softmax(a, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a : (C, N)\n",
    "        y : col major로 인코딩된 one hot (C, N)\n",
    "    \n",
    "    [참고]\n",
    "    이 함수는 미분 결과를 벡터로 만들어 되돌리는 축약형\n",
    "    원래 샘플 개수가 1개이상이면 코스트의 미분은 행렬이 되는데...\n",
    "    크로스엔트로피 함수는 출력 a로 미분을 하면 정답위치만 값이 있고 나머지 자리는 모두 0\n",
    "    그래서 행렬의 각 열에 값이 있는 부분만 모아서 벡터로 만들어 되돌림 \n",
    "    이렇게 하면 softmax와 함께 쓰일 경우 출력층 델타를 효율적으로 계산할 수 있음.\n",
    "    \"\"\"  \n",
    "    y_label = np.where(y.T==1)[1]\n",
    "    a_t = a[y_label, np.arange(y.shape[1])].reshape(-1,1) #(N,1)\n",
    "    a_t[np.where(a_t<np.finfo(np.float32).eps)] = np.finfo(np.float32).eps\n",
    "    \n",
    "    dC = -1/a_t  # dC : (N,1), a_t의 요소값이 너무 작으면 여기서 오버플로일어남. eps써서 막음\n",
    "    return dC.T # (1,N)\n",
    "\n",
    "CrossEnt = {'f':cross_entropy_cost, 'df':cross_entropy_cost_prime}\n",
    "#CrossEnt = {'f':cross_entropy_cost, 'df':cross_entropy_cost_prime_with_softmax}\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Aux function\n",
    "def one_hot_vector(y):\n",
    "    Y = np.zeros((10, y.shape[0]))   #column major \n",
    "    Y[y.T, np.arange(y.shape[0])] = 1\n",
    "    return Y\n",
    "\n",
    "#--------------------------------------------------------------------    \n",
    "# data load\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='iso-8859-1')\n",
    "f.close()\n",
    "X_train = training_data[0].reshape(50000,784).T   # column major data\n",
    "Y_train = training_data[1].reshape(50000,1)       \n",
    "X_valid = validation_data[0].reshape(10000,784).T # column major data\n",
    "Y_valid = validation_data[1].reshape(10000,1)\n",
    "X_test = test_data[0].reshape(10000,784).T  # column major data    \n",
    "Y_test = test_data[1].reshape(10000,1)\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Class\n",
    "class Network(object):\n",
    "    \"\"\"\n",
    "    네트워크 Layer 및 Index 관계\n",
    "    \n",
    "                 O\n",
    "                 O\n",
    "                 O       O\n",
    "                 O       O       O\n",
    "                 O       O       O\n",
    "                 O       O       O\n",
    "                 O       O       O\n",
    "                 O       O     \n",
    "                 O\n",
    "                 O\n",
    "    \n",
    "    layer index  0       1       2\n",
    "    ------------------------------\n",
    "    변수\n",
    "    A       [   a0      a1      a2 ] : forward시 레이어의 활성값을 보관하여 backward시 활용\n",
    "    Z       [ None      z1      z2 ] : forward시 레이어의 가중합을 보관하여 backward시 활용\n",
    "    W       [ None      w1      w2 ] : weight 행렬을 저장하는 리스트\n",
    "    dW      [ None     dw1     dw2 ] : weight에 대한 그래디언트를 저장하는 리스트\n",
    "    B       [ None      b1      b2 ] : bias 벡터를 저장하는 리스트\n",
    "    dB      [ None     db1     db2 ] : bias에 대한 그래디언트를 저장하는 리스트\n",
    "    \n",
    "    l 과 l+1을 연결하는 w에 대한 인덱스는 뒤쪽 l+1를 부여, \n",
    "    그래야 수식하고 프로그램 인덱스가 딱맞음\n",
    "    인덱스를 맞추기 위해 첫번째 엘리먼트는 None을 부여\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes, activates, cost, w_init=None ):\n",
    "        self.sizes = sizes\n",
    "        self.activates = activates\n",
    "        self.cost = cost\n",
    "        \n",
    "        # Parameters\n",
    "        if w_init == 'Xavier' :\n",
    "            self.W  = [None]+[np.random.randn(y, x)*np.sqrt(1/x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        elif w_init == 'He' :\n",
    "            self.W  = [None]+[np.random.randn(y, x)*np.sqrt(2/x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        else :\n",
    "            self.W  = [None]+[np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "            \n",
    "        self.B  = [None]+[np.random.randn(y,1) for y in sizes[1:]] \n",
    "        self.m = 0\n",
    "        self.eta = 0.01\n",
    "               \n",
    "    def pred(self, a):\n",
    "        \"\"\"\n",
    "        학습 종료 후 예측 시 forward pass를 위한 함수\n",
    "        \"\"\"\n",
    "        for l in range( 1, len(self.sizes) ) :\n",
    "            #여기서는 dropuout 비율만큼 삭감해주어야 함.\n",
    "            a = self.activates[l]['f']( (1-self.p_dropouts[l-1])*np.dot(self.W[l], a)+self.B[l] )\n",
    "        return a    \n",
    "\n",
    "    def forward(self, a):\n",
    "        \"\"\"\n",
    "        학습시 forward pass 를 위한 함수\n",
    "        \"\"\"\n",
    "        self.Z = [ None, ]\n",
    "        self.A = [ a, ]\n",
    "        self.mask = [ None, ]\n",
    "        for l in range( 1, len(sizes) ) :\n",
    "            \"\"\"\n",
    "            바이어스를 더할 때 주의 해야함. W:[nda(30,784), nad(10,30)]\n",
    "            w:nda(30,784), a:nda(784,)  이면 두개를 dot하면 nda(30,) 가 됨\n",
    "            w:nda(30,784), a:nda(784,1) 이면 두개를 dot하면 nda(30,1)가 됨\n",
    "            nda(30,)+b:nda(30,1) 하면 브로드캐스팅이 됨 \n",
    "            nda(30,1)+b:nda(30,1) 하면 nda(30,1)이 정상적으로 수행됨\n",
    "            반대로 nda(30,1)+b:nda(30,) 하면 또 브로드캐스팅 됨.\n",
    "            numpy 1차원 어레이에 행, 열 개념이 없어서 발생하는 문제..\n",
    "            결국 엄격하게 벡터의 칼럼과 로우를 구별할지 말지 결정해야함.\n",
    "            여기서는 벡터는 무조건 칼럼으로 구분하는 것으로 함. \n",
    "            \"\"\"\n",
    "            z = np.dot(self.W[l], a)+self.B[l]\n",
    "            a = self.activates[l]['f'](z)\n",
    "            \n",
    "            mask = np.random.binomial(n=1, p=1-self.p_dropouts[l], size=a.shape)\n",
    "            self.mask.append(mask)\n",
    "            a = mask*a\n",
    "            \n",
    "            self.Z.append(z)\n",
    "            self.A.append(a)\n",
    "        \n",
    "    def backward(self, y):\n",
    "        self.dB    = [ None for i in range(len(sizes)) ]\n",
    "        self.dW    = [ None for i in range(len(sizes)) ]\n",
    "                \n",
    "        Y = one_hot_vector(y)\n",
    "        L = len(sizes)-1\n",
    "        \n",
    "        for l in range(L, 0, -1) :\n",
    "            if l == L : #for delta^{L}\n",
    "                \"\"\"\n",
    "                출력 층 델타를 위해서 ∂C/∂a 와 ∂a/∂z를 Hadamard 곱을하게 되는데\n",
    "                이는 코스트함수와 출력층 활성함수가 모두 스칼라 함수일 때 가능한 방법임.\n",
    "                코스트 함수는 다변수 스칼라 함수가 되지만 출력층 활성화함수는 softmax같이 다변수 벡터함수가\n",
    "                될 수 있기 때문에 일률적으로 numpy 브로트캐스팅을 써서 곱하는 코드를 쓸 수 없음.\n",
    "                물론 Softmax-CrossEnt 를 쓰는 경우 미분 결과를 잘 정비해서 ∂C/∂a도 행렬, ∂a/∂z도 행렬로\n",
    "                만들어 주면 그냥 아래와 같은 하나의 코드로 구현 가능함.\n",
    "                \n",
    "                δ^{L} =       ∂C/∂a                 *              ∂a/∂z\n",
    "                delta  = self.cost['df'](self.A[l], Y) * self.activates[l]['df'](self.Z[l], Y)\n",
    "                \n",
    "                그렇게 구현한 함수가 \n",
    "                cross_entropy_cost_prime_with_softmax, softmax_prime_with_crossent 들임.\n",
    "                \n",
    "                그래서 출력층 델타를 구하는 코드를 일반적으로 만들기 위해서는 미분된 두항이 \n",
    "                행렬인지 텐서인지 검사해서 적절히 곱해주도록 해야함.\n",
    "                \"\"\"\n",
    "                dC , da = self.cost['df'](self.A[l], Y) , self.activates[l]['df'](self.Z[l], Y)\n",
    "                \n",
    "                if dC.shape == da.shape :\n",
    "                    delta = dC * da #∂C/∂a, ∂a/∂z 둘 모양이 같으면 그냥 Hadamard 곱, 결과:(C,N)\n",
    "                else :\n",
    "                    delta = np.einsum('ij,jik->jk', dC, da).T  #모양이 다르면 einsum 으로 곱, 결과:(C,N)\n",
    "            else : #for delta^{l}\n",
    "                delta = np.dot(self.W[l+1].T, delta)  * self.activates[l]['df'](self.Z[l], Y)\n",
    "                delta = delta*self.mask[l] # 포워드에서 저장해둔 드랍아웃 마스크를 곱한다.\n",
    "          \n",
    "            self.dB[l] = delta.sum(axis=1).reshape(-1,1)\n",
    "            self.dW[l] = np.dot(delta, self.A[l-1].T)\n",
    "    \n",
    "    def update(self):\n",
    "        self.W = [None] + [ (1-self.eta*(self.lmbda/self.n))*w-(self.eta/self.m)*nw for w, nw in zip(self.W[1:], self.dW[1:]) ]\n",
    "        self.B = [None] + [ b - (self.eta/self.m)*db for b, db in zip(self.B[1:], self.dB[1:]) ]\n",
    "                \n",
    "    def train(self, X, Y, epochs, mini_batch_size, learning_rate,  p_dropouts, lmbda=0.0, X_test=None, Y_test=None):\n",
    "        self.n = X.shape[1]          # 학습데이터 개수 \n",
    "        self.m = mini_batch_size     # 미니배치 크게\n",
    "        self.eta = learning_rate     # 학습률\n",
    "        self.p_dropouts = p_dropouts # 드랍아웃 퍼센티지\n",
    "        self.lmbda = lmbda           # 레귤러라이제이션 파라메터\n",
    "        \n",
    "        XY  = np.vstack((X, Y.T)).T  # shuffle을 위해 row major상태를 만든다.\n",
    "        train_acc, test_acc = [], []\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            np.random.shuffle(XY) #row 를 섞어 버린다.\n",
    "            mini_batches = [ XY[k:k+mini_batch_size] for k in range(0, self.n, mini_batch_size) ]\n",
    "        \n",
    "            for mini_batch in mini_batches:\n",
    "                mini_batch_X, mini_batch_Y = np.vsplit(mini_batch.T, np.array([-1])) #row major->col major, 정답인 마지막 행을 잘라 낸다.\n",
    "                mini_batch_Y = mini_batch_Y.T.astype(int) #잘려진 마지막 행은 다시 열로 바꾼다.\n",
    "                \n",
    "                self.forward(mini_batch_X)   \n",
    "                self.backward(mini_batch_Y)  \n",
    "                self.update()                \n",
    "            \n",
    "            if X_test is not None and Y_test is not None:\n",
    "                train_acc.append(self.evaluate(X, Y)/self.n*100)\n",
    "                test_acc.append(self.evaluate(X_test, Y_test)/Y_test.shape[0]*100)\n",
    "                print(\"EPOCH:{:02d} Train Acc. : {:f}, Test Acc. : {:f}\".format(j+1, train_acc[-1], test_acc[-1]))\n",
    "        \n",
    "        #plot acc\n",
    "        plt.plot(train_acc)\n",
    "        plt.plot(test_acc)\n",
    "        plt.ylim(90, 100)\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        pred = np.argmax( self.pred(X_test), axis=0 )\n",
    "        return (pred.reshape(-1,1) == Y_test).sum()\n",
    "    \n",
    "#--------------------------------------------------------------------\n",
    "# TEST\n",
    "# Layers    적당히 리스트에 숫자를 적어주면 됨 \n",
    "sizes =     [784,    100,       10]\n",
    "\n",
    "# Activation functions 각층에 할당되는 활성화 함수를 적어줌\n",
    "activates = [None,  Relu, Softmax]\n",
    "\n",
    "#dropouts 각층에 할당되는 drop out rate로 뉴런을 꺼버릴 비율을 나타낸다.\n",
    "p_dropouts  = [0.0, 0.5, 0.0]\n",
    "\n",
    "Net = Network(sizes, activates, CrossEnt, 'He')\n",
    "Net.train(X_train, Y_train, epochs=30, mini_batch_size=10, learning_rate=0.1, p_dropouts=p_dropouts, lmbda=5.0, X_test=X_test, Y_test=Y_test)\n",
    "\n",
    "a = Net.pred(X_test[:,:30])\n",
    "pred = np.argmax(a, axis=0)\n",
    "print(\"Target  : {}\".format(Y_test[:30].T[0]))\n",
    "print(\"Predict : {}\".format(pred))\n",
    "comp = pred.reshape(-1,1) == Y_test[:30]\n",
    "print(comp.sum())\n",
    "errors = np.where(comp == False)[0]\n",
    "plt.imshow(a)\n",
    "plt.show()\n",
    "\n",
    "#error rendering\n",
    "for err in errors :\n",
    "    x = X_test[:,:30][:,err]\n",
    "    x = x.reshape(28,28)\n",
    "    plt.imshow(x, cmap='gray_r')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "|    Layers    | Actvations |    Cost     | Epoches | Mini-batch | Leanrning rate | Test Acc.  | etc |\n",
    "|--------------|------------|-------------|---------|------------|----------------|-------|-----|\n",
    "| [784 150 60 10] | Relu, Relu, Softmax      | CrossEnt | 60      |   25       | 0.18            | 98.50 |Xavier, reg. $\\lambda=6.0$, max. at epoch 29 |\n",
    "| [784 150 60 10] | Relu, Relu, Softmax      | BinCrossEnt | 60      |   20       | 0.14            | 98.11 |Xavier, reg. $\\lambda=5.2$, max. at epoch 40 |\n",
    "| [784 150 60 10] | Relu, Relu, Softmax      | CrossEnt | 60      |   20       | 0.14            | 98.45 |Xavier, reg. $\\lambda=5.2$, max. at epoch 40 |\n",
    "| [784 150 60 10] | Relu, Relu, Softmax      | CrossEnt | 60      |   20       | 0.15            | 98.43 |Xavier, reg. $\\lambda=5.5$, max. at epoch 56 |\n",
    "| [784 150 60 10] | Relu, Relu, Softmax      | CrossEnt | 60      |   20       | 0.15            | 98.43 |Xavier, reg. $\\lambda=5.0$, max. at epoch 45 |\n",
    "| [784 150 60 10] | Sigmoid, Sigmoid, Softmax      | CrossEnt | 60      |   20       | 0.5            | 98.24 |Xavier, reg. $\\lambda=5.0$, max. at epoch 36 |\n",
    "| [784 100 10] | Sigmoid, Softmax      | CrossEnt | 60      |   5       | 0.1            | 98.10 |Xavier, reg. $\\lambda=5.0$, max. at epoch 58 |\n",
    "| [784 100 10] | Sigmoid, Softmax      | CrossEnt | 60      |   20       | 0.5            | 98.05 |Xavier, reg. $\\lambda=5.0$, max. at epoch 33 |\n",
    "| [784 100 10] | Sigmoid, Softmax      | CrossEnt | 60      |   20       | 0.5            | 97.80 |Xavier, max. at epoch 23 |\n",
    "| [784 100 10] | Sigmoid, Softmax      | BinCrossEnt | 60      |   5       | 0.1            | 98.13 |Xavier, reg. $\\lambda=5.0$, max. at 55 epoch |\n",
    "| [784 100 10] | Sigmoid, Sigmoid      | BinCrossEnt | 60      |   5       | 0.1            | 98.06 |Xavier, reg. $\\lambda=5.0$, max. at 45 epoch |\n",
    "| [784 100 10] | Sigmoid, Sigmoid      | BinCrossEnt | 60      |   20       | 0.5            | 97.92 |Xavier, max. at epoch 39, |\n",
    "| [784 100 10] | Sigmoid, Sigmoid      | Mse         | 60      |   20       | 3              | 98.04 |Xavier, max. at epoch 45 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이상으로 간단한 네트워크를 작성해보았다. 애초에 의도했던대로 Numpy로만 150줄내외에서 (주석지우고 softmax, cross entropy 축약버전, 그림 그리는 코드, 마지막 테스트 코드 제외하면 165줄;;;;) 구현 요구사항을 만족시키도록 작성했다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='https://fonts.googleapis.com/earlyaccess/nanummyeongjo.css' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/earlyaccess/nanumgothiccoding.css' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    p  { font-family: 'Nanum Myeongjo'; font-size: 12pt; line-height: 200%;  text-indent: 10px; }\n",
       "    li { font-family: 'Nanum Myeongjo'; font-size: 12pt; line-height: 200%; }\n",
       "    li > p { text-indent: 0px; }\n",
       "    \n",
       "    .code-body { font-family: 'Nanum Gothic Coding', serif; }\n",
       "    \n",
       "    code,highlight { font-family: 'Nanum Gothic Coding', serif; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link href='https://fonts.googleapis.com/earlyaccess/nanummyeongjo.css' rel='stylesheet' type='text/css'>\n",
    "<link href='http://fonts.googleapis.com/earlyaccess/nanumgothiccoding.css' rel='stylesheet' type='text/css'>\n",
    "<style>\n",
    "    p  { font-family: 'Nanum Myeongjo'; font-size: 12pt; line-height: 200%;  text-indent: 10px; }\n",
    "    li { font-family: 'Nanum Myeongjo'; font-size: 12pt; line-height: 200%; }\n",
    "    li > p { text-indent: 0px; }\n",
    "    \n",
    "    .code-body { font-family: 'Nanum Gothic Coding', serif; }\n",
    "    \n",
    "    code, highlight { font-family: 'Nanum Gothic Coding', serif; }\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
